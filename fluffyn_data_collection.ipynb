{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOQ7NJubpXh_",
        "outputId": "e08dbc2c-ddfc-4ba8-9969-f4a4699eccf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading SQL dump from 'breed_details_backup.sql'...\n",
            "Cleaning SQL for SQLite compatibility...\n",
            "Executing SQL script in-memory...\n",
            "Found tables: ['dog_breed_details', 'cat_breed_details']\n",
            "Extracting and mapping data from table 'dog_breed_details'...\n",
            "Extracting and mapping data from table 'cat_breed_details'...\n",
            "Writing all data to lookup JSON file 'pet_database.json'...\n",
            "\n",
            "Conversion complete!\n",
            "Output saved to 'pet_database.json'\n"
          ]
        }
      ],
      "source": [
        "import sqlite3\n",
        "import re\n",
        "import json\n",
        "\n",
        "def clean_mysql_dump_for_sqlite(sql_content):\n",
        "    \"\"\"\n",
        "    Cleans a MySQL dump to make it compatible with SQLite.\n",
        "    This version has more robust regular expressions to prevent syntax errors.\n",
        "    \"\"\"\n",
        "    # Remove MySQL-specific comment blocks like /*!40101 ... */;\n",
        "    # This regex is now more robust.\n",
        "    sql_content = re.sub(r'/\\*!.*?\\*/;?', '', sql_content, flags=re.DOTALL)\n",
        "\n",
        "    # --- FIXED PART ---\n",
        "    # A single, more robust regex to remove LOCK/UNLOCK TABLES commands,\n",
        "    # ignoring case and handling potential whitespace.\n",
        "    sql_content = re.sub(r'^\\s*(lock|unlock)\\s+tables.*?;', '', sql_content,\n",
        "                         flags=re.IGNORECASE | re.MULTILINE)\n",
        "\n",
        "    # Remove MySQL-specific table options (ENGINE, CHARSET, COLLATE, etc.)\n",
        "    sql_content = re.sub(r'\\)\\s*ENGINE=.*?;', ');', sql_content, flags=re.DOTALL | re.IGNORECASE)\n",
        "\n",
        "    # Convert MySQL `datetime(3)` to SQLite-compatible DATETIME\n",
        "    sql_content = sql_content.replace('datetime(3)', 'DATETIME')\n",
        "\n",
        "    # Convert MySQL `tinyint(1)` to SQLite-compatible INTEGER\n",
        "    sql_content = sql_content.replace('tinyint(1)', 'INTEGER')\n",
        "\n",
        "    # Remove extra KEY definitions that are not PRIMARY KEY\n",
        "    sql_content = re.sub(r',?\\s+KEY `.*?` \\(.*?`\\)', '', sql_content)\n",
        "\n",
        "    return sql_content\n",
        "\n",
        "def convert_sql_to_lookup_json(sql_file_path, json_file_path):\n",
        "    \"\"\"\n",
        "    Reads a MySQL dump, loads it into an in-memory SQLite database,\n",
        "    and exports all breeds into a single JSON object keyed by breed name.\n",
        "    \"\"\"\n",
        "    print(f\"Reading SQL dump from '{sql_file_path}'...\")\n",
        "    try:\n",
        "        with open(sql_file_path, 'r', encoding='utf-8') as f:\n",
        "            sql_dump = f.read()\n",
        "    except FileNotFoundError:\n",
        "        print(f\"ERROR: The file '{sql_file_path}' was not found. Please make sure it's in the same directory.\")\n",
        "        return\n",
        "\n",
        "    print(\"Cleaning SQL for SQLite compatibility...\")\n",
        "    cleaned_sql = clean_mysql_dump_for_sqlite(sql_dump)\n",
        "\n",
        "    # For debugging, you can print the cleaned SQL to a file\n",
        "    # with open('cleaned_debug.sql', 'w') as f:\n",
        "    #     f.write(cleaned_sql)\n",
        "\n",
        "    conn = sqlite3.connect(':memory:')\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    print(\"Executing SQL script in-memory...\")\n",
        "    try:\n",
        "        cursor.executescript(cleaned_sql)\n",
        "        conn.commit()\n",
        "    except sqlite3.Error as e:\n",
        "        print(f\"An error occurred while executing the SQL script: {e}\")\n",
        "        print(\"This likely means the cleaning function missed some MySQL-specific syntax.\")\n",
        "        conn.close()\n",
        "        return\n",
        "\n",
        "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "    tables = [row[0] for row in cursor.fetchall()]\n",
        "    if not tables:\n",
        "        print(\"No tables were created. The SQL script might be empty or failed silently.\")\n",
        "        conn.close()\n",
        "        return\n",
        "    print(f\"Found tables: {tables}\")\n",
        "\n",
        "    pet_lookup_db = {}\n",
        "    conn.row_factory = sqlite3.Row\n",
        "\n",
        "    for table_name in tables:\n",
        "        pet_type = 'unknown'\n",
        "        if 'dog' in table_name:\n",
        "            pet_type = 'dog'\n",
        "        elif 'cat' in table_name:\n",
        "            pet_type = 'cat'\n",
        "\n",
        "        print(f\"Extracting and mapping data from table '{table_name}'...\")\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute(f\"SELECT * FROM {table_name}\")\n",
        "        rows = cursor.fetchall()\n",
        "\n",
        "        for row in rows:\n",
        "            pet_data = dict(row)\n",
        "            breed_name = pet_data.pop('breed_name', None)\n",
        "            if not breed_name:\n",
        "                continue\n",
        "\n",
        "            pet_data['pet_type'] = pet_type\n",
        "            pet_lookup_db[breed_name] = pet_data\n",
        "\n",
        "    conn.close()\n",
        "\n",
        "    print(f\"Writing all data to lookup JSON file '{json_file_path}'...\")\n",
        "    with open(json_file_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(pet_lookup_db, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "    print(\"\\nConversion complete!\")\n",
        "    print(f\"Output saved to '{json_file_path}'\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    sql_input_file = 'breed_details_backup.sql' # Using your file name\n",
        "    json_output_file = 'pet_database.json'\n",
        "    convert_sql_to_lookup_json(sql_input_file, json_output_file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def simplify_pet_data(pet_name, data):\n",
        "    \"\"\"Simplifies and cleans the data for a single pet breed.\"\"\"\n",
        "\n",
        "    # 1. Define all keys to be completely removed\n",
        "    keys_to_remove = [\n",
        "        'created_at', 'updated_at', 'deleted_at', 'id',\n",
        "        'full_front', 'full_left', 'full_right', 'face_front', 'photo5'\n",
        "    ]\n",
        "\n",
        "    # 2. Define keys that will be consolidated into simpler fields\n",
        "    #    We will remove these after processing them.\n",
        "    price_keys = [\n",
        "        'male_min_price', 'male_max_price', 'female_min_price', 'female_max_price',\n",
        "        'low_quality_male_min_price', 'low_quality_male_max_price', 'medium_quality_male_min_price',\n",
        "        'medium_quality_male_max_price', 'show_quality_male_min_price', 'show_quality_male_max_price',\n",
        "        'champion_quality_male_min_price', 'champion_quality_male_max_price', 'low_quality_female_min_price',\n",
        "        'low_quality_female_max_price', 'medium_quality_female_min_price', 'medium_quality_female_max_price',\n",
        "        'show_quality_female_min_price', 'show_quality_female_max_price', 'champion_quality_female_min_price',\n",
        "        'champion_quality_female_max_price'\n",
        "    ]\n",
        "    weight_keys = [\n",
        "        'male_min_full_grown_weight', 'male_max_full_grown_weight',\n",
        "        'female_min_full_grown_weight', 'female_max_full_grown_weight'\n",
        "    ]\n",
        "    height_keys = [\n",
        "        'male_min_full_grown_height', 'male_max_full_grown_height',\n",
        "        'female_min_full_grown_height', 'female_max_full_grown_height'\n",
        "    ]\n",
        "    lifespan_keys = ['min_life_span', 'max_life_span']\n",
        "    litter_keys = ['min_litter_size', 'max_litter_size']\n",
        "\n",
        "    # Start with a copy of the original data\n",
        "    clean_data = data.copy()\n",
        "\n",
        "    # 3. Create simplified, human-readable fields\n",
        "\n",
        "    # --- Simplify Price ---\n",
        "    all_prices = [v for k, v in data.items() if k in price_keys and v > 0]\n",
        "    if all_prices:\n",
        "        min_price = min(all_prices)\n",
        "        max_price = max(all_prices)\n",
        "        # Using Indian formatting for currency\n",
        "        clean_data['price_range'] = f\"‚Çπ{min_price:,} - ‚Çπ{max_price:,}\"\n",
        "\n",
        "    # --- Simplify Weight ---\n",
        "    if data.get('male_max_full_grown_weight'):\n",
        "        m_min_w = data['male_min_full_grown_weight']\n",
        "        m_max_w = data['male_max_full_grown_weight']\n",
        "        f_min_w = data['female_min_full_grown_weight']\n",
        "        f_max_w = data['female_max_full_grown_weight']\n",
        "        clean_data['weight'] = f\"Male: {m_min_w}-{m_max_w} kg, Female: {f_min_w}-{f_max_w} kg\"\n",
        "\n",
        "    # --- Simplify Height ---\n",
        "    if data.get('male_max_full_grown_height'):\n",
        "        m_min_h = data['male_min_full_grown_height']\n",
        "        m_max_h = data['male_max_full_grown_height']\n",
        "        f_min_h = data['female_min_full_grown_height']\n",
        "        f_max_h = data['female_max_full_grown_height']\n",
        "        clean_data['height'] = f\"Male: {m_min_h}-{m_max_h} cm, Female: {f_min_h}-{f_max_h} cm\"\n",
        "\n",
        "    # --- Simplify Life Span ---\n",
        "    if data.get('max_life_span'):\n",
        "        clean_data['life_span'] = f\"{data['min_life_span']} - {data['max_life_span']} years\"\n",
        "\n",
        "    # --- Simplify Litter Size ---\n",
        "    if data.get('max_litter_size', 0) > 0:\n",
        "        unit = \"puppies\" if data.get('pet_type') == 'dog' else \"kittens\"\n",
        "        clean_data['litter_size'] = f\"{data['min_litter_size']} - {data['max_litter_size']} {unit}\"\n",
        "\n",
        "    # 4. Remove all processed and unnecessary keys\n",
        "    keys_to_purge = keys_to_remove + price_keys + weight_keys + height_keys + lifespan_keys + litter_keys\n",
        "    for key in keys_to_purge:\n",
        "        clean_data.pop(key, None) # Use .pop with a default to avoid errors\n",
        "\n",
        "    return clean_data\n",
        "\n",
        "\n",
        "def main():\n",
        "    input_file = 'pet_database.json'\n",
        "    output_file = 'chatbot_training_data.json'\n",
        "\n",
        "    print(f\"Loading data from '{input_file}'...\")\n",
        "    try:\n",
        "        with open(input_file, 'r', encoding='utf-8') as f:\n",
        "            full_db = json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"ERROR: Input file '{input_file}' not found. Please run the previous script first.\")\n",
        "        return\n",
        "\n",
        "    print(\"Processing and cleaning data for chatbot...\")\n",
        "\n",
        "    cleaned_db = {}\n",
        "    for pet_name, pet_data in full_db.items():\n",
        "        cleaned_db[pet_name] = simplify_pet_data(pet_name, pet_data)\n",
        "\n",
        "    print(f\"Saving cleaned data to '{output_file}'...\")\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(cleaned_db, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "    print(\"\\nProcess complete!\")\n",
        "    print(f\"A clean dataset for your chatbot has been saved to '{output_file}'.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlObjSt6p3V8",
        "outputId": "06b9deb9-bd1a-4bd0-c964-4857cc0e3511"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data from 'pet_database.json'...\n",
            "Processing and cleaning data for chatbot...\n",
            "Saving cleaned data to 'chatbot_training_data.json'...\n",
            "\n",
            "Process complete!\n",
            "A clean dataset for your chatbot has been saved to 'chatbot_training_data.json'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install beautifulsoup4 requests google-generativeai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXDRPvwaqAZc",
        "outputId": "6b22bf56-631b-4994-d2f8-b88466c02f59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.7.14)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.25.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.177.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.1)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.74.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Pet Dataset Enrichment Pipeline - 2-Level Data Enhancement System\n",
        "==================================================================\n",
        "\n",
        "This notebook implements a sophisticated 2-level pipeline to enrich pet breed data:\n",
        "Level 1: Web Scraping from authoritative sources (AKC, Wikipedia)\n",
        "Level 2: LLM enrichment using Google's Gemini REST API for intelligent data generation\n",
        "\n",
        "Features Added: 22 new comprehensive features including health, training, history, and lifestyle data\n",
        "\n",
        "IMPROVEMENTS:\n",
        "- Enhanced rate limiting with exponential backoff\n",
        "- Better JSON parsing with fallback mechanisms\n",
        "- Retry logic for failed requests\n",
        "- Progress saving and resume capability\n",
        "\"\"\"\n",
        "\n",
        "# =============================================================================\n",
        "# SETUP AND INSTALLATION\n",
        "# =============================================================================\n",
        "\n",
        "# Install required packages\n",
        "# !pip install -q beautifulsoup4 requests lxml html5lib\n",
        "\n",
        "import json\n",
        "import time\n",
        "import os\n",
        "import re\n",
        "import random\n",
        "from typing import Dict, List, Optional, Any\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import quote_plus\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# =============================================================================\n",
        "# CONFIGURATION AND CONSTANTS\n",
        "# =============================================================================\n",
        "\n",
        "# File paths\n",
        "INPUT_FILE = 'chatbot_training_data.json'\n",
        "OUTPUT_FILE = 'final_enriched_training_data.json'\n",
        "PROGRESS_FILE = 'pipeline_progress.json'\n",
        "\n",
        "# Using the recommended latest stable model\n",
        "GEMINI_MODEL_NAME = 'gemini-1.5-flash-latest'\n",
        "\n",
        "# Complete list of 22 new features to be added\n",
        "NEW_FEATURES_ADDED = [\n",
        "    # Health & Wellness (5 features)\n",
        "    \"common_health_concerns\",\n",
        "    \"health_disclaimer\",\n",
        "    \"recommended_health_tests\",\n",
        "    \"general_dietary_needs\",\n",
        "    \"average_exercise_needs\",\n",
        "\n",
        "    # Training & Behavior (6 features)\n",
        "    \"training_difficulty\",\n",
        "    \"training_tips\",\n",
        "    \"socialization_needs\",\n",
        "    \"common_behavioral_issues\",\n",
        "    \"mental_stimulation_needs\",\n",
        "    \"prey_drive_level\",\n",
        "\n",
        "    # Breed History & Characteristics (5 features)\n",
        "    \"breed_history\",\n",
        "    \"breed_group\",\n",
        "    \"puppy_availability\",\n",
        "    \"distinguishing_features\",\n",
        "    \"celebrity_owners\",\n",
        "\n",
        "    # Lifestyle & Home Compatibility (6 features)\n",
        "    \"good_for_first_time_owners\",\n",
        "    \"ideal_living_conditions\",\n",
        "    \"tolerance_to_being_alone\",\n",
        "    \"weather_tolerance_details\",\n",
        "    \"grooming_frequency_and_tips\",\n",
        "    \"cost_of_ownership_summary\"\n",
        "]\n",
        "\n",
        "# Request headers to avoid being blocked\n",
        "HEADERS = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "}\n",
        "\n",
        "# Rate limiting configuration\n",
        "RATE_LIMIT_CONFIG = {\n",
        "    'base_delay': 3,  # Base delay between requests (seconds)\n",
        "    'max_delay': 60,  # Maximum delay for exponential backoff\n",
        "    'max_retries': 5,  # Maximum number of retries\n",
        "    'backoff_factor': 2  # Exponential backoff multiplier\n",
        "}\n",
        "\n",
        "# =============================================================================\n",
        "# ENHANCED UTILITIES\n",
        "# =============================================================================\n",
        "\n",
        "class ProgressTracker:\n",
        "    \"\"\"Track and save pipeline progress to enable resume functionality\"\"\"\n",
        "    def __init__(self, progress_file: str):\n",
        "        self.progress_file = progress_file\n",
        "        self.data = self._load_progress()\n",
        "\n",
        "    def _load_progress(self) -> Dict:\n",
        "        \"\"\"Load existing progress or create new\"\"\"\n",
        "        try:\n",
        "            with open(self.progress_file, 'r') as f:\n",
        "                return json.load(f)\n",
        "        except FileNotFoundError:\n",
        "            return {\n",
        "                'completed_breeds': [],\n",
        "                'failed_breeds': [],\n",
        "                'last_updated': None,\n",
        "                'total_processed': 0\n",
        "            }\n",
        "\n",
        "    def save_progress(self):\n",
        "        \"\"\"Save current progress\"\"\"\n",
        "        self.data['last_updated'] = datetime.now().isoformat()\n",
        "        with open(self.progress_file, 'w') as f:\n",
        "            json.dump(self.data, f, indent=2)\n",
        "\n",
        "    def mark_completed(self, breed_name: str):\n",
        "        \"\"\"Mark a breed as completed\"\"\"\n",
        "        if breed_name not in self.data['completed_breeds']:\n",
        "            self.data['completed_breeds'].append(breed_name)\n",
        "            self.data['total_processed'] += 1\n",
        "            self.save_progress()\n",
        "\n",
        "    def mark_failed(self, breed_name: str):\n",
        "        \"\"\"Mark a breed as failed\"\"\"\n",
        "        if breed_name not in self.data['failed_breeds']:\n",
        "            self.data['failed_breeds'].append(breed_name)\n",
        "            self.save_progress()\n",
        "\n",
        "    def is_completed(self, breed_name: str) -> bool:\n",
        "        \"\"\"Check if breed was already processed\"\"\"\n",
        "        return breed_name in self.data['completed_breeds']\n",
        "\n",
        "    def get_remaining_breeds(self, all_breeds: List[str]) -> List[str]:\n",
        "        \"\"\"Get list of breeds that still need processing\"\"\"\n",
        "        return [breed for breed in all_breeds if not self.is_completed(breed)]\n",
        "\n",
        "class RateLimiter:\n",
        "    \"\"\"Enhanced rate limiter with exponential backoff\"\"\"\n",
        "    def __init__(self, config: Dict):\n",
        "        self.config = config\n",
        "        self.last_request_time = 0\n",
        "        self.consecutive_failures = 0\n",
        "\n",
        "    def wait_if_needed(self):\n",
        "        \"\"\"Wait appropriate amount of time before next request\"\"\"\n",
        "        current_time = time.time()\n",
        "        time_since_last = current_time - self.last_request_time\n",
        "\n",
        "        # Calculate delay based on consecutive failures\n",
        "        if self.consecutive_failures > 0:\n",
        "            delay = min(\n",
        "                self.config['base_delay'] * (self.config['backoff_factor'] ** self.consecutive_failures),\n",
        "                self.config['max_delay']\n",
        "            )\n",
        "        else:\n",
        "            delay = self.config['base_delay']\n",
        "\n",
        "        if time_since_last < delay:\n",
        "            wait_time = delay - time_since_last\n",
        "            print(f\"  ‚è≥ Rate limiting: waiting {wait_time:.1f} seconds...\")\n",
        "            time.sleep(wait_time)\n",
        "\n",
        "        self.last_request_time = time.time()\n",
        "\n",
        "    def record_success(self):\n",
        "        \"\"\"Record successful request\"\"\"\n",
        "        self.consecutive_failures = 0\n",
        "\n",
        "    def record_failure(self):\n",
        "        \"\"\"Record failed request\"\"\"\n",
        "        self.consecutive_failures += 1\n",
        "\n",
        "# =============================================================================\n",
        "# GEMINI API KEY SETUP\n",
        "# =============================================================================\n",
        "\n",
        "def get_gemini_api_key():\n",
        "    \"\"\"Gets the Gemini API key from environment variables or user input.\"\"\"\n",
        "    print(\"üîß Getting Gemini API key...\")\n",
        "    api_key = os.environ.get(\"GOOGLE_API_KEY\")\n",
        "    if not api_key:\n",
        "        print(\"‚ö†Ô∏è GOOGLE_API_KEY not found in environment variables.\")\n",
        "        print(\"Please get your free API key from: https://makersuite.google.com/app/apikey\")\n",
        "        api_key = input(\"Enter your Gemini API key: \").strip()\n",
        "        if not api_key:\n",
        "            raise ValueError(\"API key is required to proceed\")\n",
        "    print(\"‚úÖ Gemini API key loaded.\")\n",
        "    return api_key\n",
        "\n",
        "# =============================================================================\n",
        "# LEVEL 1: WEB SCRAPING FUNCTIONS (Enhanced with better error handling)\n",
        "# =============================================================================\n",
        "\n",
        "class WebScraper:\n",
        "    \"\"\"Advanced web scraper for pet breed information\"\"\"\n",
        "    def __init__(self):\n",
        "        self.session = requests.Session()\n",
        "        self.session.headers.update(HEADERS)\n",
        "        self.rate_limiter = RateLimiter({'base_delay': 1, 'max_delay': 10, 'backoff_factor': 1.5, 'max_retries': 3})\n",
        "\n",
        "    def scrape_akc_data(self, breed_name: str) -> Dict[str, str]:\n",
        "        \"\"\"Scrape American Kennel Club data for a breed\"\"\"\n",
        "        print(f\"  üîç [AKC Scraper] Searching for '{breed_name}'...\")\n",
        "        search_name = breed_name.lower().replace(' ', '-').replace(\"'\", \"\")\n",
        "        url = f\"https://www.akc.org/dog-breeds/{search_name}/\"\n",
        "\n",
        "        self.rate_limiter.wait_if_needed()\n",
        "\n",
        "        try:\n",
        "            response = self.session.get(url, timeout=10)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.content, 'lxml')\n",
        "            scraped_data = {}\n",
        "\n",
        "            for section_key, keywords in {\n",
        "                'breed_group': ['breed-group'],\n",
        "                'health': ['health', 'care'],\n",
        "                'training': ['training', 'personality', 'temperament'],\n",
        "                'history': ['history', 'origin'],\n",
        "                'grooming': ['grooming', 'coat']\n",
        "            }.items():\n",
        "                if section_key == 'breed_group':\n",
        "                    elem = soup.find('span', class_='breed-group')\n",
        "                    if elem:\n",
        "                        scraped_data['breed_group'] = elem.get_text(strip=True)\n",
        "                else:\n",
        "                    content = self._find_section_content(soup, keywords)\n",
        "                    if content:\n",
        "                        scraped_data[section_key] = content[:1000]\n",
        "\n",
        "            print(f\"  ‚úÖ [AKC] Found {len(scraped_data)} sections for '{breed_name}'\")\n",
        "            self.rate_limiter.record_success()\n",
        "            return scraped_data\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"  ‚ö†Ô∏è [AKC] Network error for '{breed_name}': {e}\")\n",
        "            self.rate_limiter.record_failure()\n",
        "            return {}\n",
        "\n",
        "    def scrape_wikipedia_data(self, breed_name: str) -> Dict[str, str]:\n",
        "        \"\"\"Scrape Wikipedia data for additional breed information\"\"\"\n",
        "        print(f\"  üîç [Wikipedia] Searching for '{breed_name}'...\")\n",
        "        url = f\"https://en.wikipedia.org/wiki/{quote_plus(breed_name.replace(' ', '_'))}\"\n",
        "\n",
        "        self.rate_limiter.wait_if_needed()\n",
        "\n",
        "        try:\n",
        "            response = self.session.get(url, timeout=10)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.content, 'lxml')\n",
        "            scraped_data = {}\n",
        "\n",
        "            content_div = soup.find('div', {'class': 'mw-parser-output'})\n",
        "            if content_div:\n",
        "                paragraphs = content_div.find_all('p', recursive=False)[:3]\n",
        "                general_info = ' '.join([p.get_text(strip=True) for p in paragraphs])\n",
        "                if general_info:\n",
        "                    scraped_data['wikipedia_info'] = re.sub(r'\\[.*?\\]', '', general_info)[:1500]\n",
        "\n",
        "            print(f\"  ‚úÖ [Wikipedia] Found general info for '{breed_name}'\")\n",
        "            self.rate_limiter.record_success()\n",
        "            return scraped_data\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ö†Ô∏è [Wikipedia] Error for '{breed_name}': {e}\")\n",
        "            self.rate_limiter.record_failure()\n",
        "            return {}\n",
        "\n",
        "    def _find_section_content(self, soup: BeautifulSoup, keywords: List[str]) -> Optional[str]:\n",
        "        \"\"\"Find content sections based on header keywords\"\"\"\n",
        "        for keyword in keywords:\n",
        "            header = soup.find(['h1', 'h2', 'h3', 'h4'], string=re.compile(keyword, re.IGNORECASE))\n",
        "            if header:\n",
        "                content_elements = []\n",
        "                for sibling in header.find_next_siblings():\n",
        "                    if sibling.name in ['h1', 'h2', 'h3', 'h4']:\n",
        "                        break\n",
        "                    if sibling.name == 'p':\n",
        "                        content_elements.append(sibling.get_text(strip=True))\n",
        "                if content_elements:\n",
        "                    return ' '.join(content_elements)\n",
        "        return None\n",
        "\n",
        "# =============================================================================\n",
        "# LEVEL 2: ENHANCED GEMINI LLM ENRICHMENT\n",
        "# =============================================================================\n",
        "\n",
        "class GeminiEnricher:\n",
        "    \"\"\"Enhanced LLM-powered data enrichment using Google Gemini REST API\"\"\"\n",
        "    def __init__(self, api_key: str):\n",
        "        self.api_key = api_key\n",
        "        self.api_url = f\"https://generativelanguage.googleapis.com/v1beta/models/{GEMINI_MODEL_NAME}:generateContent\"\n",
        "        self.rate_limiter = RateLimiter(RATE_LIMIT_CONFIG)\n",
        "\n",
        "    def enrich_breed_data(self, breed_name: str, original_data: Dict, scraped_data: Dict) -> Dict[str, Any]:\n",
        "        \"\"\"Use Gemini REST API to generate comprehensive breed features with retry logic\"\"\"\n",
        "        print(f\"  ü§ñ [Gemini] Enriching data for '{breed_name}'...\")\n",
        "\n",
        "        for attempt in range(RATE_LIMIT_CONFIG['max_retries']):\n",
        "            try:\n",
        "                self.rate_limiter.wait_if_needed()\n",
        "\n",
        "                prompt = self._create_enrichment_prompt(breed_name, original_data, scraped_data)\n",
        "                payload = {\"contents\": [{\"parts\": [{\"text\": prompt}]}]}\n",
        "                headers = {'Content-Type': 'application/json'}\n",
        "                params = {'key': self.api_key}\n",
        "\n",
        "                response = requests.post(\n",
        "                    self.api_url,\n",
        "                    headers=headers,\n",
        "                    params=params,\n",
        "                    json=payload,\n",
        "                    timeout=60\n",
        "                )\n",
        "\n",
        "                if response.status_code == 429:\n",
        "                    print(f\"  ‚ö†Ô∏è [Gemini] Rate limit hit (attempt {attempt + 1}/{RATE_LIMIT_CONFIG['max_retries']})\")\n",
        "                    self.rate_limiter.record_failure()\n",
        "                    if attempt < RATE_LIMIT_CONFIG['max_retries'] - 1:\n",
        "                        wait_time = RATE_LIMIT_CONFIG['base_delay'] * (RATE_LIMIT_CONFIG['backoff_factor'] ** attempt)\n",
        "                        print(f\"  ‚è≥ Waiting {wait_time} seconds before retry...\")\n",
        "                        time.sleep(wait_time)\n",
        "                        continue\n",
        "                    else:\n",
        "                        raise requests.exceptions.RequestException(\"Max retries exceeded for rate limiting\")\n",
        "\n",
        "                response.raise_for_status()\n",
        "\n",
        "                response_json = response.json()\n",
        "                content_text = response_json['candidates'][0]['content']['parts'][0]['text']\n",
        "\n",
        "                enriched_data = self._parse_gemini_response(content_text)\n",
        "                if enriched_data:\n",
        "                    print(f\"  ‚úÖ [Gemini] Successfully enriched '{breed_name}' with {len(enriched_data)} features\")\n",
        "                    self.rate_limiter.record_success()\n",
        "                    return enriched_data\n",
        "                else:\n",
        "                    raise ValueError(\"Parsed data is empty\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  ‚ùå [Gemini] Attempt {attempt + 1} failed for '{breed_name}': {e}\")\n",
        "                self.rate_limiter.record_failure()\n",
        "                if attempt < RATE_LIMIT_CONFIG['max_retries'] - 1:\n",
        "                    wait_time = RATE_LIMIT_CONFIG['base_delay'] * (RATE_LIMIT_CONFIG['backoff_factor'] ** attempt)\n",
        "                    print(f\"  ‚è≥ Retrying in {wait_time} seconds...\")\n",
        "                    time.sleep(wait_time)\n",
        "                else:\n",
        "                    print(f\"  ‚ùå [Gemini] All attempts failed for '{breed_name}', using fallback data\")\n",
        "                    return self._create_fallback_data(breed_name)\n",
        "\n",
        "        return self._create_fallback_data(breed_name)\n",
        "\n",
        "    def _create_enrichment_prompt(self, breed_name: str, original_data: Dict, scraped_data: Dict) -> str:\n",
        "        \"\"\"Creates the detailed prompt for the Gemini model.\"\"\"\n",
        "        return f\"\"\"You are an expert pet data analyst creating a comprehensive breed profile.\n",
        "\n",
        "BREED: {breed_name}\n",
        "\n",
        "EXISTING DATA: {json.dumps(original_data, indent=2)}\n",
        "SCRAPED WEB DATA: {json.dumps(scraped_data, indent=2)}\n",
        "\n",
        "Create a JSON object with these exact keys: {json.dumps(NEW_FEATURES_ADDED)}\n",
        "\n",
        "CRITICAL REQUIREMENTS:\n",
        "1. Return ONLY valid JSON - no markdown, no extra text\n",
        "2. Use \"health_disclaimer\": \"This information is not a substitute for professional veterinary advice. Please consult a vet for any health issues.\"\n",
        "3. For boolean fields, use true/false (not strings)\n",
        "4. For arrays, provide actual arrays like [\"item1\", \"item2\"] or [] if empty\n",
        "5. Keep responses concise but informative\n",
        "\n",
        "OUTPUT FORMAT: Return only the JSON object, nothing else.\"\"\"\n",
        "\n",
        "    def _parse_gemini_response(self, response_text: str) -> Optional[Dict]:\n",
        "        \"\"\"Enhanced JSON parsing with multiple fallback strategies\"\"\"\n",
        "        # Clean the response text\n",
        "        cleaned_text = response_text.strip()\n",
        "\n",
        "        # Remove markdown code blocks if present\n",
        "        if cleaned_text.startswith('```json'):\n",
        "            cleaned_text = cleaned_text[7:]\n",
        "        if cleaned_text.startswith('```'):\n",
        "            cleaned_text = cleaned_text[3:]\n",
        "        if cleaned_text.endswith('```'):\n",
        "            cleaned_text = cleaned_text[:-3]\n",
        "\n",
        "        cleaned_text = cleaned_text.strip()\n",
        "\n",
        "        # Try to find JSON within the text\n",
        "        json_start = cleaned_text.find('{')\n",
        "        json_end = cleaned_text.rfind('}')\n",
        "\n",
        "        if json_start != -1 and json_end != -1 and json_end > json_start:\n",
        "            cleaned_text = cleaned_text[json_start:json_end+1]\n",
        "\n",
        "        try:\n",
        "            data = json.loads(cleaned_text)\n",
        "\n",
        "            # Ensure all required keys are present\n",
        "            missing_keys = set(NEW_FEATURES_ADDED) - set(data.keys())\n",
        "            if missing_keys:\n",
        "                print(f\"  ‚ö†Ô∏è [Parser] Missing keys: {missing_keys}. Filling with defaults.\")\n",
        "                for key in missing_keys:\n",
        "                    data[key] = self._get_default_value(key)\n",
        "\n",
        "            # Clean up any extra keys\n",
        "            data = {key: data[key] for key in NEW_FEATURES_ADDED if key in data}\n",
        "\n",
        "            return data\n",
        "\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"  ‚ùå [Parser] JSON parsing failed: {e}\")\n",
        "            print(f\"  üìÑ Response preview: {cleaned_text[:200]}...\")\n",
        "            return None\n",
        "\n",
        "    def _get_default_value(self, key: str) -> Any:\n",
        "        \"\"\"Get appropriate default value for a key\"\"\"\n",
        "        if key == \"health_disclaimer\":\n",
        "            return \"This information is not a substitute for professional veterinary advice. Please consult a vet for any health issues.\"\n",
        "        elif key in [\"recommended_health_tests\", \"common_health_concerns\"]:\n",
        "            return []\n",
        "        elif key == \"good_for_first_time_owners\":\n",
        "            return \"Unknown - consult with breed experts\"\n",
        "        else:\n",
        "            return \"Information not available\"\n",
        "\n",
        "    def _create_fallback_data(self, breed_name: str) -> Dict[str, Any]:\n",
        "        \"\"\"Create fallback data structure when Gemini fails completely\"\"\"\n",
        "        return {key: self._get_default_value(key) for key in NEW_FEATURES_ADDED}\n",
        "\n",
        "# =============================================================================\n",
        "# ENHANCED MAIN PIPELINE CLASS\n",
        "# =============================================================================\n",
        "\n",
        "class PetDatasetPipeline:\n",
        "    \"\"\"Enhanced main pipeline orchestrator with progress tracking\"\"\"\n",
        "    def __init__(self):\n",
        "        self.scraper = WebScraper()\n",
        "        self.api_key = get_gemini_api_key()\n",
        "        self.enricher = GeminiEnricher(self.api_key)\n",
        "        self.progress_tracker = ProgressTracker(PROGRESS_FILE)\n",
        "        self.processed_count = 0\n",
        "        self.failed_count = 0\n",
        "\n",
        "    def load_input_data(self, file_path: str) -> Dict:\n",
        "        \"\"\"Load and validate input data\"\"\"\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "            print(f\"üìÇ Loaded {len(data)} breeds from '{file_path}'\")\n",
        "            return data\n",
        "        except FileNotFoundError:\n",
        "            print(f\"‚ùå Input file '{file_path}' not found! Please upload it.\")\n",
        "            return {}\n",
        "\n",
        "    def process_breed(self, breed_name: str, original_data: Dict) -> Dict:\n",
        "        \"\"\"Process a single breed through the complete pipeline\"\"\"\n",
        "        print(f\"\\nüîÑ Processing: {breed_name}\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        try:\n",
        "            print(\"üì° LEVEL 1: Web Scraping\")\n",
        "            akc_data = self.scraper.scrape_akc_data(breed_name)\n",
        "            wikipedia_data = self.scraper.scrape_wikipedia_data(breed_name)\n",
        "            all_scraped_data = {**akc_data, **wikipedia_data}\n",
        "\n",
        "            print(\"ü§ñ LEVEL 2: LLM Enrichment\")\n",
        "            enriched_features = self.enricher.enrich_breed_data(\n",
        "                breed_name, original_data, all_scraped_data\n",
        "            )\n",
        "\n",
        "            final_data = {**original_data, **enriched_features}\n",
        "            self.processed_count += 1\n",
        "            self.progress_tracker.mark_completed(breed_name)\n",
        "            print(f\"‚úÖ Successfully processed '{breed_name}'\")\n",
        "            return final_data\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Top-level failure processing '{breed_name}': {e}\")\n",
        "            self.failed_count += 1\n",
        "            self.progress_tracker.mark_failed(breed_name)\n",
        "            return original_data\n",
        "\n",
        "    def run_pipeline(self, input_file: str, output_file: str, limit: Optional[int] = None):\n",
        "        \"\"\"Run the complete pipeline with resume capability\"\"\"\n",
        "        print(\"üöÄ STARTING PET DATASET ENRICHMENT PIPELINE\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        input_data = self.load_input_data(input_file)\n",
        "        if not input_data:\n",
        "            return\n",
        "\n",
        "        all_breeds = list(input_data.keys())\n",
        "        if limit:\n",
        "            all_breeds = all_breeds[:limit]\n",
        "\n",
        "        # Check for resumable progress\n",
        "        remaining_breeds = self.progress_tracker.get_remaining_breeds(all_breeds)\n",
        "        already_completed = len(all_breeds) - len(remaining_breeds)\n",
        "\n",
        "        if already_completed > 0:\n",
        "            print(f\"üìã Resuming pipeline: {already_completed} breeds already completed\")\n",
        "            print(f\"üìã Will process {len(remaining_breeds)} remaining breeds\")\n",
        "        else:\n",
        "            print(f\"üìã Will process {len(remaining_breeds)} breeds\")\n",
        "\n",
        "        # Load existing results if available\n",
        "        final_dataset = {}\n",
        "        try:\n",
        "            with open(output_file, 'r', encoding='utf-8') as f:\n",
        "                final_dataset = json.load(f)\n",
        "                print(f\"üìÇ Loaded existing results: {len(final_dataset)} breeds\")\n",
        "        except FileNotFoundError:\n",
        "            print(\"üìÇ Starting fresh - no existing results found\")\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        for i, breed_name in enumerate(remaining_breeds, 1):\n",
        "            print(f\"\\nüìä Progress: {i}/{len(remaining_breeds)} (Total: {already_completed + i}/{len(all_breeds)})\")\n",
        "            final_dataset[breed_name] = self.process_breed(breed_name, input_data[breed_name])\n",
        "\n",
        "            # Save intermediate results every 5 breeds\n",
        "            if i % 5 == 0:\n",
        "                self._save_results(final_dataset, output_file)\n",
        "                print(f\"üíæ Intermediate save completed ({i} breeds processed)\")\n",
        "\n",
        "        self._save_results(final_dataset, output_file)\n",
        "        elapsed_time = time.time() - start_time\n",
        "\n",
        "        print(\"\\nüéâ PIPELINE COMPLETED!\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"‚è±Ô∏è  Total time: {elapsed_time:.1f} seconds\")\n",
        "        print(f\"‚úÖ Successfully processed: {self.processed_count} breeds\")\n",
        "        print(f\"‚ùå Failed: {self.failed_count} breeds\")\n",
        "        print(f\"üíæ Final dataset saved to: '{output_file}'\")\n",
        "\n",
        "    def _save_results(self, dataset: Dict, output_file: str):\n",
        "        \"\"\"Save the enriched dataset\"\"\"\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(dataset, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "# =============================================================================\n",
        "# MAIN EXECUTION\n",
        "# =============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"READY TO START PIPELINE\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"\\nChoose an option:\")\n",
        "    print(\"1. Run demo (3 breeds) - Recommended for testing\")\n",
        "    print(\"2. Run full pipeline (all breeds)\")\n",
        "    print(\"3. Resume interrupted pipeline\")\n",
        "\n",
        "    choice = input(\"\\nEnter your choice (1, 2, or 3): \").strip()\n",
        "\n",
        "    pipeline = PetDatasetPipeline()\n",
        "\n",
        "    if choice == \"1\":\n",
        "        pipeline.run_pipeline(INPUT_FILE, 'demo_' + OUTPUT_FILE, limit=3)\n",
        "    elif choice == \"2\":\n",
        "        pipeline.run_pipeline(INPUT_FILE, OUTPUT_FILE)\n",
        "    elif choice == \"3\":\n",
        "        pipeline.run_pipeline(INPUT_FILE, OUTPUT_FILE)\n",
        "    else:\n",
        "        print(\"‚ùå Invalid choice. Running demo by default.\")\n",
        "        pipeline.run_pipeline(INPUT_FILE, 'demo_' + OUTPUT_FILE, limit=3)\n",
        "\n",
        "    print(\"\\nüéâ Script completed! Check the output files for results.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EDZklKRqqoB",
        "outputId": "cb904537-4795-4085-b29e-f2e52909a89e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "READY TO START PIPELINE\n",
            "============================================================\n",
            "\n",
            "Choose an option:\n",
            "1. Run demo (3 breeds) - Recommended for testing\n",
            "2. Run full pipeline (all breeds)\n",
            "3. Resume interrupted pipeline\n",
            "\n",
            "Enter your choice (1, 2, or 3): 2\n",
            "üîß Getting Gemini API key...\n",
            "‚ö†Ô∏è GOOGLE_API_KEY not found in environment variables.\n",
            "Please get your free API key from: https://makersuite.google.com/app/apikey\n",
            "Enter your Gemini API key: AIzaSyADI--hSVZgSqFBKFH4Mbc-i-bCglLvTAE\n",
            "‚úÖ Gemini API key loaded.\n",
            "üöÄ STARTING PET DATASET ENRICHMENT PIPELINE\n",
            "============================================================\n",
            "üìÇ Loaded 45 breeds from 'chatbot_training_data.json'\n",
            "üìã Will process 45 breeds\n",
            "üìÇ Starting fresh - no existing results found\n",
            "\n",
            "üìä Progress: 1/45 (Total: 1/45)\n",
            "\n",
            "üîÑ Processing: Rottweiler\n",
            "==================================================\n",
            "üì° LEVEL 1: Web Scraping\n",
            "  üîç [AKC Scraper] Searching for 'Rottweiler'...\n",
            "  ‚úÖ [AKC] Found 0 sections for 'Rottweiler'\n",
            "  üîç [Wikipedia] Searching for 'Rottweiler'...\n",
            "  ‚è≥ Rate limiting: waiting 0.8 seconds...\n",
            "  ‚úÖ [Wikipedia] Found general info for 'Rottweiler'\n",
            "ü§ñ LEVEL 2: LLM Enrichment\n",
            "  ü§ñ [Gemini] Enriching data for 'Rottweiler'...\n",
            "  ‚úÖ [Gemini] Successfully enriched 'Rottweiler' with 22 features\n",
            "‚úÖ Successfully processed 'Rottweiler'\n",
            "\n",
            "üìä Progress: 2/45 (Total: 2/45)\n",
            "\n",
            "üîÑ Processing: Lhasa Apso\n",
            "==================================================\n",
            "üì° LEVEL 1: Web Scraping\n",
            "  üîç [AKC Scraper] Searching for 'Lhasa Apso'...\n",
            "  ‚úÖ [AKC] Found 0 sections for 'Lhasa Apso'\n",
            "  üîç [Wikipedia] Searching for 'Lhasa Apso'...\n",
            "  ‚è≥ Rate limiting: waiting 0.9 seconds...\n",
            "  ‚úÖ [Wikipedia] Found general info for 'Lhasa Apso'\n",
            "ü§ñ LEVEL 2: LLM Enrichment\n",
            "  ü§ñ [Gemini] Enriching data for 'Lhasa Apso'...\n",
            "  ‚úÖ [Gemini] Successfully enriched 'Lhasa Apso' with 22 features\n",
            "‚úÖ Successfully processed 'Lhasa Apso'\n",
            "\n",
            "üìä Progress: 3/45 (Total: 3/45)\n",
            "\n",
            "üîÑ Processing: Beagle\n",
            "==================================================\n",
            "üì° LEVEL 1: Web Scraping\n",
            "  üîç [AKC Scraper] Searching for 'Beagle'...\n",
            "  ‚úÖ [AKC] Found 0 sections for 'Beagle'\n",
            "  üîç [Wikipedia] Searching for 'Beagle'...\n",
            "  ‚è≥ Rate limiting: waiting 0.9 seconds...\n",
            "  ‚úÖ [Wikipedia] Found general info for 'Beagle'\n",
            "ü§ñ LEVEL 2: LLM Enrichment\n",
            "  ü§ñ [Gemini] Enriching data for 'Beagle'...\n",
            "  ‚úÖ [Gemini] Successfully enriched 'Beagle' with 22 features\n",
            "‚úÖ Successfully processed 'Beagle'\n",
            "\n",
            "üìä Progress: 4/45 (Total: 4/45)\n",
            "\n",
            "üîÑ Processing: American Bulldog\n",
            "==================================================\n",
            "üì° LEVEL 1: Web Scraping\n",
            "  üîç [AKC Scraper] Searching for 'American Bulldog'...\n",
            "  ‚úÖ [AKC] Found 0 sections for 'American Bulldog'\n",
            "  üîç [Wikipedia] Searching for 'American Bulldog'...\n",
            "  ‚è≥ Rate limiting: waiting 0.9 seconds...\n",
            "  ‚úÖ [Wikipedia] Found general info for 'American Bulldog'\n",
            "ü§ñ LEVEL 2: LLM Enrichment\n",
            "  ü§ñ [Gemini] Enriching data for 'American Bulldog'...\n",
            "  ‚úÖ [Gemini] Successfully enriched 'American Bulldog' with 22 features\n",
            "‚úÖ Successfully processed 'American Bulldog'\n",
            "\n",
            "üìä Progress: 5/45 (Total: 5/45)\n",
            "\n",
            "üîÑ Processing: Bull Mastiff\n",
            "==================================================\n",
            "üì° LEVEL 1: Web Scraping\n",
            "  üîç [AKC Scraper] Searching for 'Bull Mastiff'...\n",
            "  ‚ö†Ô∏è [AKC] Network error for 'Bull Mastiff': 404 Client Error: Not Found for url: https://www.akc.org/dog-breeds/bull-mastiff/\n",
            "  üîç [Wikipedia] Searching for 'Bull Mastiff'...\n",
            "  ‚è≥ Rate limiting: waiting 0.6 seconds...\n",
            "  ‚úÖ [Wikipedia] Found general info for 'Bull Mastiff'\n",
            "ü§ñ LEVEL 2: LLM Enrichment\n",
            "  ü§ñ [Gemini] Enriching data for 'Bull Mastiff'...\n",
            "  ‚úÖ [Gemini] Successfully enriched 'Bull Mastiff' with 22 features\n",
            "‚úÖ Successfully processed 'Bull Mastiff'\n",
            "üíæ Intermediate save completed (5 breeds processed)\n",
            "\n",
            "üìä Progress: 6/45 (Total: 6/45)\n",
            "\n",
            "üîÑ Processing: Chow Chow\n",
            "==================================================\n",
            "üì° LEVEL 1: Web Scraping\n",
            "  üîç [AKC Scraper] Searching for 'Chow Chow'...\n",
            "  ‚úÖ [AKC] Found 0 sections for 'Chow Chow'\n",
            "  üîç [Wikipedia] Searching for 'Chow Chow'...\n",
            "  ‚è≥ Rate limiting: waiting 0.9 seconds...\n",
            "  ‚úÖ [Wikipedia] Found general info for 'Chow Chow'\n",
            "ü§ñ LEVEL 2: LLM Enrichment\n",
            "  ü§ñ [Gemini] Enriching data for 'Chow Chow'...\n",
            "  ‚ùå [Gemini] Attempt 1 failed for 'Chow Chow': 503 Server Error: Service Unavailable for url: https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyADI--hSVZgSqFBKFH4Mbc-i-bCglLvTAE\n",
            "  ‚è≥ Retrying in 3 seconds...\n",
            "  ‚è≥ Rate limiting: waiting 1.6 seconds...\n",
            "  ‚úÖ [Gemini] Successfully enriched 'Chow Chow' with 22 features\n",
            "‚úÖ Successfully processed 'Chow Chow'\n",
            "\n",
            "üìä Progress: 7/45 (Total: 7/45)\n",
            "\n",
            "üîÑ Processing: German Shepherd\n",
            "==================================================\n",
            "üì° LEVEL 1: Web Scraping\n",
            "  üîç [AKC Scraper] Searching for 'German Shepherd'...\n",
            "  ‚úÖ [AKC] Found 0 sections for 'German Shepherd'\n",
            "  üîç [Wikipedia] Searching for 'German Shepherd'...\n",
            "  ‚è≥ Rate limiting: waiting 0.8 seconds...\n",
            "  ‚úÖ [Wikipedia] Found general info for 'German Shepherd'\n",
            "ü§ñ LEVEL 2: LLM Enrichment\n",
            "  ü§ñ [Gemini] Enriching data for 'German Shepherd'...\n",
            "  ‚úÖ [Gemini] Successfully enriched 'German Shepherd' with 22 features\n",
            "‚úÖ Successfully processed 'German Shepherd'\n",
            "\n",
            "üìä Progress: 8/45 (Total: 8/45)\n",
            "\n",
            "üîÑ Processing: Alaskan Malamute\n",
            "==================================================\n",
            "üì° LEVEL 1: Web Scraping\n",
            "  üîç [AKC Scraper] Searching for 'Alaskan Malamute'...\n",
            "  ‚úÖ [AKC] Found 0 sections for 'Alaskan Malamute'\n",
            "  üîç [Wikipedia] Searching for 'Alaskan Malamute'...\n",
            "  ‚è≥ Rate limiting: waiting 0.9 seconds...\n",
            "  ‚úÖ [Wikipedia] Found general info for 'Alaskan Malamute'\n",
            "ü§ñ LEVEL 2: LLM Enrichment\n",
            "  ü§ñ [Gemini] Enriching data for 'Alaskan Malamute'...\n",
            "  ‚ùå [Gemini] Attempt 1 failed for 'Alaskan Malamute': 503 Server Error: Service Unavailable for url: https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyADI--hSVZgSqFBKFH4Mbc-i-bCglLvTAE\n",
            "  ‚è≥ Retrying in 3 seconds...\n",
            "  ‚è≥ Rate limiting: waiting 0.2 seconds...\n",
            "  ‚úÖ [Gemini] Successfully enriched 'Alaskan Malamute' with 22 features\n",
            "‚úÖ Successfully processed 'Alaskan Malamute'\n",
            "\n",
            "üìä Progress: 9/45 (Total: 9/45)\n",
            "\n",
            "üîÑ Processing: Siberian Husky\n",
            "==================================================\n",
            "üì° LEVEL 1: Web Scraping\n",
            "  üîç [AKC Scraper] Searching for 'Siberian Husky'...\n",
            "  ‚úÖ [AKC] Found 0 sections for 'Siberian Husky'\n",
            "  üîç [Wikipedia] Searching for 'Siberian Husky'...\n",
            "  ‚è≥ Rate limiting: waiting 0.9 seconds...\n",
            "  ‚úÖ [Wikipedia] Found general info for 'Siberian Husky'\n",
            "ü§ñ LEVEL 2: LLM Enrichment\n",
            "  ü§ñ [Gemini] Enriching data for 'Siberian Husky'...\n",
            "  ‚úÖ [Gemini] Successfully enriched 'Siberian Husky' with 22 features\n",
            "‚úÖ Successfully processed 'Siberian Husky'\n",
            "\n",
            "üìä Progress: 10/45 (Total: 10/45)\n",
            "\n",
            "üîÑ Processing: Corgi\n",
            "==================================================\n",
            "üì° LEVEL 1: Web Scraping\n",
            "  üîç [AKC Scraper] Searching for 'Corgi'...\n",
            "  ‚ö†Ô∏è [AKC] Network error for 'Corgi': 404 Client Error: Not Found for url: https://www.akc.org/dog-breeds/corgi/\n",
            "  üîç [Wikipedia] Searching for 'Corgi'...\n",
            "  ‚è≥ Rate limiting: waiting 0.7 seconds...\n",
            "  ‚úÖ [Wikipedia] Found general info for 'Corgi'\n",
            "ü§ñ LEVEL 2: LLM Enrichment\n",
            "  ü§ñ [Gemini] Enriching data for 'Corgi'...\n",
            "  ‚úÖ [Gemini] Successfully enriched 'Corgi' with 22 features\n",
            "‚úÖ Successfully processed 'Corgi'\n",
            "üíæ Intermediate save completed (10 breeds processed)\n",
            "\n",
            "üìä Progress: 11/45 (Total: 11/45)\n",
            "\n",
            "üîÑ Processing: Chihuahua\n",
            "==================================================\n",
            "üì° LEVEL 1: Web Scraping\n",
            "  üîç [AKC Scraper] Searching for 'Chihuahua'...\n",
            "  ‚úÖ [AKC] Found 0 sections for 'Chihuahua'\n",
            "  üîç [Wikipedia] Searching for 'Chihuahua'...\n",
            "  ‚è≥ Rate limiting: waiting 0.9 seconds...\n",
            "  ‚úÖ [Wikipedia] Found general info for 'Chihuahua'\n",
            "ü§ñ LEVEL 2: LLM Enrichment\n",
            "  ü§ñ [Gemini] Enriching data for 'Chihuahua'...\n",
            "  ‚úÖ [Gemini] Successfully enriched 'Chihuahua' with 22 features\n",
            "‚úÖ Successfully processed 'Chihuahua'\n",
            "\n",
            "üìä Progress: 12/45 (Total: 12/45)\n",
            "\n",
            "üîÑ Processing: Tibetan Mastiff\n",
            "==================================================\n",
            "üì° LEVEL 1: Web Scraping\n",
            "  üîç [AKC Scraper] Searching for 'Tibetan Mastiff'...\n",
            "  ‚úÖ [AKC] Found 0 sections for 'Tibetan Mastiff'\n",
            "  üîç [Wikipedia] Searching for 'Tibetan Mastiff'...\n",
            "  ‚è≥ Rate limiting: waiting 0.9 seconds...\n",
            "  ‚úÖ [Wikipedia] Found general info for 'Tibetan Mastiff'\n",
            "ü§ñ LEVEL 2: LLM Enrichment\n",
            "  ü§ñ [Gemini] Enriching data for 'Tibetan Mastiff'...\n",
            "  ‚úÖ [Gemini] Successfully enriched 'Tibetan Mastiff' with 22 features\n",
            "‚úÖ Successfully processed 'Tibetan Mastiff'\n",
            "\n",
            "üìä Progress: 13/45 (Total: 13/45)\n",
            "\n",
            "üîÑ Processing: Pomeranian\n",
            "==================================================\n",
            "üì° LEVEL 1: Web Scraping\n",
            "  üîç [AKC Scraper] Searching for 'Pomeranian'...\n",
            "  ‚úÖ [AKC] Found 0 sections for 'Pomeranian'\n",
            "  üîç [Wikipedia] Searching for 'Pomeranian'...\n",
            "  ‚è≥ Rate limiting: waiting 0.9 seconds...\n",
            "  ‚úÖ [Wikipedia] Found general info for 'Pomeranian'\n",
            "ü§ñ LEVEL 2: LLM Enrichment\n",
            "  ü§ñ [Gemini] Enriching data for 'Pomeranian'...\n",
            "  ‚úÖ [Gemini] Successfully enriched 'Pomeranian' with 22 features\n",
            "‚úÖ Successfully processed 'Pomeranian'\n",
            "\n",
            "üìä Progress: 14/45 (Total: 14/45)\n",
            "\n",
            "üîÑ Processing: Golden Retriever\n",
            "==================================================\n",
            "üì° LEVEL 1: Web Scraping\n",
            "  üîç [AKC Scraper] Searching for 'Golden Retriever'...\n",
            "  ‚úÖ [AKC] Found 0 sections for 'Golden Retriever'\n",
            "  üîç [Wikipedia] Searching for 'Golden Retriever'...\n",
            "  ‚è≥ Rate limiting: waiting 0.9 seconds...\n",
            "  ‚úÖ [Wikipedia] Found general info for 'Golden Retriever'\n",
            "ü§ñ LEVEL 2: LLM Enrichment\n",
            "  ü§ñ [Gemini] Enriching data for 'Golden Retriever'...\n",
            "  ‚úÖ [Gemini] Successfully enriched 'Golden Retriever' with 22 features\n",
            "‚úÖ Successfully processed 'Golden Retriever'\n",
            "\n",
            "üìä Progress: 15/45 (Total: 15/45)\n",
            "\n",
            "üîÑ Processing: French Mastiff\n",
            "==================================================\n",
            "üì° LEVEL 1: Web Scraping\n",
            "  üîç [AKC Scraper] Searching for 'French Mastiff'...\n",
            "  ‚ö†Ô∏è [AKC] Network error for 'French Mastiff': 404 Client Error: Not Found for url: https://www.akc.org/dog-breeds/french-mastiff/\n",
            "  üîç [Wikipedia] Searching for 'French Mastiff'...\n",
            "  ‚è≥ Rate limiting: waiting 0.5 seconds...\n",
            "  ‚úÖ [Wikipedia] Found general info for 'French Mastiff'\n",
            "ü§ñ LEVEL 2: LLM Enrichment\n",
            "  ü§ñ [Gemini] Enriching data for 'French Mastiff'...\n",
            "  ‚úÖ [Gemini] Successfully enriched 'French Mastiff' with 22 features\n",
            "‚úÖ Successfully processed 'French Mastiff'\n",
            "üíæ Intermediate save completed (15 breeds processed)\n",
            "\n",
            "üìä Progress: 16/45 (Total: 16/45)\n",
            "\n",
            "üîÑ Processing: English Mastiff\n",
            "==================================================\n",
            "üì° LEVEL 1: Web Scraping\n",
            "  üîç [AKC Scraper] Searching for 'English Mastiff'...\n",
            "  ‚ö†Ô∏è [AKC] Network error for 'English Mastiff': 404 Client Error: Not Found for url: https://www.akc.org/dog-breeds/english-mastiff/\n",
            "  üîç [Wikipedia] Searching for 'English Mastiff'...\n",
            "  ‚è≥ Rate limiting: waiting 0.3 seconds...\n",
            "  ‚úÖ [Wikipedia] Found general info for 'English Mastiff'\n",
            "ü§ñ LEVEL 2: LLM Enrichment\n",
            "  ü§ñ [Gemini] Enriching data for 'English Mastiff'...\n",
            "  ‚úÖ [Gemini] Successfully enriched 'English Mastiff' with 22 features\n",
            "‚úÖ Successfully processed 'English Mastiff'\n",
            "\n",
            "üìä Progress: 17/45 (Total: 17/45)\n",
            "\n",
            "üîÑ Processing: Saint Bernard\n",
            "==================================================\n",
            "üì° LEVEL 1: Web Scraping\n",
            "  üîç [AKC Scraper] Searching for 'Saint Bernard'...\n",
            "  ‚úÖ [AKC] Found 0 sections for 'Saint Bernard'\n",
            "  üîç [Wikipedia] Searching for 'Saint Bernard'...\n",
            "  ‚è≥ Rate limiting: waiting 0.9 seconds...\n",
            "  ‚úÖ [Wikipedia] Found general info for 'Saint Bernard'\n",
            "ü§ñ LEVEL 2: LLM Enrichment\n",
            "  ü§ñ [Gemini] Enriching data for 'Saint Bernard'...\n",
            "  ‚úÖ [Gemini] Successfully enriched 'Saint Bernard' with 22 features\n",
            "‚úÖ Successfully processed 'Saint Bernard'\n",
            "\n",
            "üìä Progress: 18/45 (Total: 18/45)\n",
            "\n",
            "üîÑ Processing: Cane Corso\n",
            "==================================================\n",
            "üì° LEVEL 1: Web Scraping\n",
            "  üîç [AKC Scraper] Searching for 'Cane Corso'...\n",
            "  ‚úÖ [AKC] Found 0 sections for 'Cane Corso'\n",
            "  üîç [Wikipedia] Searching for 'Cane Corso'...\n",
            "  ‚è≥ Rate limiting: waiting 0.9 seconds...\n",
            "  ‚úÖ [Wikipedia] Found general info for 'Cane Corso'\n",
            "ü§ñ LEVEL 2: LLM Enrichment\n",
            "  ü§ñ [Gemini] Enriching data for 'Cane Corso'...\n",
            "  ‚úÖ [Gemini] Successfully enriched 'Cane Corso' with 22 features\n",
            "‚úÖ Successfully processed 'Cane Corso'\n",
            "\n",
            "üìä Progress: 19/45 (Total: 19/45)\n",
            "\n",
            "üîÑ Processing: Shih Tzu\n",
            "==================================================\n",
            "üì° LEVEL 1: Web Scraping\n",
            "  üîç [AKC Scraper] Searching for 'Shih Tzu'...\n",
            "  ‚úÖ [AKC] Found 0 sections for 'Shih Tzu'\n",
            "  üîç [Wikipedia] Searching for 'Shih Tzu'...\n",
            "  ‚è≥ Rate limiting: waiting 0.9 seconds...\n",
            "  ‚úÖ [Wikipedia] Found general info for 'Shih Tzu'\n",
            "ü§ñ LEVEL 2: LLM Enrichment\n",
            "  ü§ñ [Gemini] Enriching data for 'Shih Tzu'...\n",
            "  ‚úÖ [Gemini] Successfully enriched 'Shih Tzu' with 22 features\n",
            "‚úÖ Successfully processed 'Shih Tzu'\n",
            "\n",
            "üìä Progress: 20/45 (Total: 20/45)\n",
            "\n",
            "üîÑ Processing: Border Collie\n",
            "==================================================\n",
            "üì° LEVEL 1: Web Scraping\n",
            "  üîç [AKC Scraper] Searching for 'Border Collie'...\n",
            "  ‚úÖ [AKC] Found 0 sections for 'Border Collie'\n",
            "  üîç [Wikipedia] Searching for 'Border Collie'...\n",
            "  ‚è≥ Rate limiting: waiting 0.9 seconds...\n",
            "  ‚úÖ [Wikipedia] Found general info for 'Border Collie'\n",
            "ü§ñ LEVEL 2: LLM Enrichment\n",
            "  ü§ñ [Gemini] Enriching data for 'Border Collie'...\n",
            "  ‚ùå [Gemini] Attempt 1 failed for 'Border Collie': 503 Server Error: Service Unavailable for url: https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyADI--hSVZgSqFBKFH4Mbc-i-bCglLvTAE\n",
            "  ‚è≥ Retrying in 3 seconds...\n",
            "  ‚è≥ Rate limiting: waiting 2.9 seconds...\n",
            "  ‚úÖ [Gemini] Successfully enriched 'Border Collie' with 22 features\n",
            "‚úÖ Successfully processed 'Border Collie'\n",
            "üíæ Intermediate save completed (20 breeds processed)\n",
            "\n",
            "üìä Progress: 21/45 (Total: 21/45)\n",
            "\n",
            "üîÑ Processing: Great Dane\n",
            "==================================================\n",
            "üì° LEVEL 1: Web Scraping\n",
            "  üîç [AKC Scraper] Searching for 'Great Dane'...\n",
            "  ‚úÖ [AKC] Found 0 sections for 'Great Dane'\n",
            "  üîç [Wikipedia] Searching for 'Great Dane'...\n",
            "  ‚è≥ Rate limiting: waiting 0.9 seconds...\n",
            "  ‚úÖ [Wikipedia] Found general info for 'Great Dane'\n",
            "ü§ñ LEVEL 2: LLM Enrichment\n",
            "  ü§ñ [Gemini] Enriching data for 'Great Dane'...\n",
            "  ‚úÖ [Gemini] Successfully enriched 'Great Dane' with 22 features\n",
            "‚úÖ Successfully processed 'Great Dane'\n",
            "\n",
            "üìä Progress: 22/45 (Total: 22/45)\n",
            "\n",
            "üîÑ Processing: Cocker Spaniel\n",
            "==================================================\n",
            "üì° LEVEL 1: Web Scraping\n",
            "  üîç [AKC Scraper] Searching for 'Cocker Spaniel'...\n",
            "  ‚úÖ [AKC] Found 0 sections for 'Cocker Spaniel'\n",
            "  üîç [Wikipedia] Searching for 'Cocker Spaniel'...\n",
            "  ‚è≥ Rate limiting: waiting 0.9 seconds...\n",
            "  ‚úÖ [Wikipedia] Found general info for 'Cocker Spaniel'\n",
            "ü§ñ LEVEL 2: LLM Enrichment\n",
            "  ü§ñ [Gemini] Enriching data for 'Cocker Spaniel'...\n",
            "  ‚ùå [Gemini] Attempt 1 failed for 'Cocker Spaniel': 503 Server Error: Service Unavailable for url: https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyADI--hSVZgSqFBKFH4Mbc-i-bCglLvTAE\n",
            "  ‚è≥ Retrying in 3 seconds...\n",
            "  ‚è≥ Rate limiting: waiting 2.7 seconds...\n",
            "  ‚úÖ [Gemini] Successfully enriched 'Cocker Spaniel' with 22 features\n",
            "‚úÖ Successfully processed 'Cocker Spaniel'\n",
            "\n",
            "üìä Progress: 23/45 (Total: 23/45)\n",
            "\n",
            "üîÑ Processing: French Bulldog\n",
            "==================================================\n",
            "üì° LEVEL 1: Web Scraping\n",
            "  üîç [AKC Scraper] Searching for 'French Bulldog'...\n",
            "  ‚úÖ [AKC] Found 0 sections for 'French Bulldog'\n",
            "  üîç [Wikipedia] Searching for 'French Bulldog'...\n",
            "  ‚è≥ Rate limiting: waiting 0.9 seconds...\n",
            "  ‚úÖ [Wikipedia] Found general info for 'French Bulldog'\n",
            "ü§ñ LEVEL 2: LLM Enrichment\n",
            "  ü§ñ [Gemini] Enriching data for 'French Bulldog'...\n",
            "  ‚úÖ [Gemini] Successfully enriched 'French Bulldog' with 22 features\n",
            "‚úÖ Successfully processed 'French Bulldog'\n",
            "\n",
            "üìä Progress: 24/45 (Total: 24/45)\n",
            "\n",
            "üîÑ Processing: Labrador Retriever\n",
            "==================================================\n",
            "üì° LEVEL 1: Web Scraping\n",
            "  üîç [AKC Scraper] Searching for 'Labrador Retriever'...\n",
            "  ‚úÖ [AKC] Found 0 sections for 'Labrador Retriever'\n",
            "  üîç [Wikipedia] Searching for 'Labrador Retriever'...\n",
            "  ‚è≥ Rate limiting: waiting 0.9 seconds...\n",
            "  ‚úÖ [Wikipedia] Found general info for 'Labrador Retriever'\n",
            "ü§ñ LEVEL 2: LLM Enrichment\n",
            "  ü§ñ [Gemini] Enriching data for 'Labrador Retriever'...\n",
            "  ‚úÖ [Gemini] Successfully enriched 'Labrador Retriever' with 22 features\n",
            "‚úÖ Successfully processed 'Labrador Retriever'\n",
            "\n",
            "üìä Progress: 25/45 (Total: 25/45)\n",
            "\n",
            "üîÑ Processing: English Bulldog\n",
            "==================================================\n",
            "üì° LEVEL 1: Web Scraping\n",
            "  üîç [AKC Scraper] Searching for 'English Bulldog'...\n",
            "  ‚úÖ [AKC] Found 0 sections for 'English Bulldog'\n",
            "  üîç [Wikipedia] Searching for 'English Bulldog'...\n",
            "  ‚è≥ Rate limiting: waiting 0.8 seconds...\n",
            "  ‚úÖ [Wikipedia] Found general info for 'English Bulldog'\n",
            "ü§ñ LEVEL 2: LLM Enrichment\n",
            "  ü§ñ [Gemini] Enriching data for 'English Bulldog'...\n",
            "  ‚úÖ [Gemini] Successfully enriched 'English Bulldog' with 22 features\n",
            "‚úÖ Successfully processed 'English Bulldog'\n",
            "üíæ Intermediate save completed (25 breeds processed)\n",
            "\n",
            "üìä Progress: 26/45 (Total: 26/45)\n",
            "\n",
            "üîÑ Processing: Shiba Inu\n",
            "==================================================\n",
            "üì° LEVEL 1: Web Scraping\n",
            "  üîç [AKC Scraper] Searching for 'Shiba Inu'...\n",
            "  ‚úÖ [AKC] Found 0 sections for 'Shiba Inu'\n",
            "  üîç [Wikipedia] Searching for 'Shiba Inu'...\n",
            "  ‚è≥ Rate limiting: waiting 0.9 seconds...\n",
            "  ‚úÖ [Wikipedia] Found general info for 'Shiba Inu'\n",
            "ü§ñ LEVEL 2: LLM Enrichment\n",
            "  ü§ñ [Gemini] Enriching data for 'Shiba Inu'...\n",
            "  ‚ùå [Gemini] Attempt 1 failed for 'Shiba Inu': 503 Server Error: Service Unavailable for url: https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyADI--hSVZgSqFBKFH4Mbc-i-bCglLvTAE\n",
            "  ‚è≥ Retrying in 3 seconds...\n",
            "  ‚è≥ Rate limiting: waiting 2.3 seconds...\n",
            "  ‚úÖ [Gemini] Successfully enriched 'Shiba Inu' with 22 features\n",
            "‚úÖ Successfully processed 'Shiba Inu'\n",
            "\n",
            "üìä Progress: 27/45 (Total: 27/45)\n",
            "\n",
            "üîÑ Processing: Pug\n",
            "==================================================\n",
            "üì° LEVEL 1: Web Scraping\n",
            "  üîç [AKC Scraper] Searching for 'Pug'...\n",
            "  ‚úÖ [AKC] Found 0 sections for 'Pug'\n",
            "  üîç [Wikipedia] Searching for 'Pug'...\n",
            "  ‚è≥ Rate limiting: waiting 0.9 seconds...\n",
            "  ‚úÖ [Wikipedia] Found general info for 'Pug'\n",
            "ü§ñ LEVEL 2: LLM Enrichment\n",
            "  ü§ñ [Gemini] Enriching data for 'Pug'...\n",
            "  ‚úÖ [Gemini] Successfully enriched 'Pug' with 22 features\n",
            "‚úÖ Successfully processed 'Pug'\n",
            "\n",
            "üìä Progress: 28/45 (Total: 28/45)\n",
            "\n",
            "üîÑ Processing: Neapolitan Mastiff\n",
            "==================================================\n",
            "üì° LEVEL 1: Web Scraping\n",
            "  üîç [AKC Scraper] Searching for 'Neapolitan Mastiff'...\n",
            "  ‚úÖ [AKC] Found 0 sections for 'Neapolitan Mastiff'\n",
            "  üîç [Wikipedia] Searching for 'Neapolitan Mastiff'...\n",
            "  ‚è≥ Rate limiting: waiting 0.9 seconds...\n",
            "  ‚úÖ [Wikipedia] Found general info for 'Neapolitan Mastiff'\n",
            "ü§ñ LEVEL 2: LLM Enrichment\n",
            "  ü§ñ [Gemini] Enriching data for 'Neapolitan Mastiff'...\n",
            "  ‚úÖ [Gemini] Successfully enriched 'Neapolitan Mastiff' with 22 features\n",
            "‚úÖ Successfully processed 'Neapolitan Mastiff'\n",
            "\n",
            "üìä Progress: 29/45 (Total: 29/45)\n",
            "\n",
            "üîÑ Processing: Maltese\n",
            "==================================================\n",
            "üì° LEVEL 1: Web Scraping\n",
            "  üîç [AKC Scraper] Searching for 'Maltese'...\n",
            "  ‚úÖ [AKC] Found 0 sections for 'Maltese'\n",
            "  üîç [Wikipedia] Searching for 'Maltese'...\n",
            "  ‚è≥ Rate limiting: waiting 0.9 seconds...\n",
            "  ‚úÖ [Wikipedia] Found general info for 'Maltese'\n",
            "ü§ñ LEVEL 2: LLM Enrichment\n",
            "  ü§ñ [Gemini] Enriching data for 'Maltese'...\n",
            "  ‚úÖ [Gemini] Successfully enriched 'Maltese' with 22 features\n",
            "‚úÖ Successfully processed 'Maltese'\n",
            "\n",
            "üìä Progress: 30/45 (Total: 30/45)\n",
            "\n",
            "üîÑ Processing: Poodle\n",
            "==================================================\n",
            "üì° LEVEL 1: Web Scraping\n",
            "  üîç [AKC Scraper] Searching for 'Poodle'...\n",
            "  ‚úÖ [AKC] Found 0 sections for 'Poodle'\n",
            "  üîç [Wikipedia] Searching for 'Poodle'...\n",
            "  ‚è≥ Rate limiting: waiting 0.9 seconds...\n",
            "  ‚úÖ [Wikipedia] Found general info for 'Poodle'\n",
            "ü§ñ LEVEL 2: LLM Enrichment\n",
            "  ü§ñ [Gemini] Enriching data for 'Poodle'...\n",
            "  ‚ùå [Gemini] Attempt 1 failed for 'Poodle': 503 Server Error: Service Unavailable for url: https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyADI--hSVZgSqFBKFH4Mbc-i-bCglLvTAE\n",
            "  ‚è≥ Retrying in 3 seconds...\n",
            "  ‚è≥ Rate limiting: waiting 2.9 seconds...\n",
            "  ‚ùå [Gemini] Attempt 2 failed for 'Poodle': 503 Server Error: Service Unavailable for url: https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyADI--hSVZgSqFBKFH4Mbc-i-bCglLvTAE\n",
            "  ‚è≥ Retrying in 6 seconds...\n",
            "  ‚è≥ Rate limiting: waiting 5.9 seconds...\n",
            "  ‚úÖ [Gemini] Successfully enriched 'Poodle' with 22 features\n",
            "‚úÖ Successfully processed 'Poodle'\n",
            "üíæ Intermediate save completed (30 breeds processed)\n",
            "\n",
            "üìä Progress: 31/45 (Total: 31/45)\n",
            "\n",
            "üîÑ Processing: Doberman Pinscher\n",
            "==================================================\n",
            "üì° LEVEL 1: Web Scraping\n",
            "  üîç [AKC Scraper] Searching for 'Doberman Pinscher'...\n",
            "  ‚úÖ [AKC] Found 0 sections for 'Doberman Pinscher'\n",
            "  üîç [Wikipedia] Searching for 'Doberman Pinscher'...\n",
            "  ‚è≥ Rate limiting: waiting 0.9 seconds...\n",
            "  ‚úÖ [Wikipedia] Found general info for 'Doberman Pinscher'\n",
            "ü§ñ LEVEL 2: LLM Enrichment\n",
            "  ü§ñ [Gemini] Enriching data for 'Doberman Pinscher'...\n",
            "  ‚ùå [Gemini] Attempt 1 failed for 'Doberman Pinscher': 503 Server Error: Service Unavailable for url: https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyADI--hSVZgSqFBKFH4Mbc-i-bCglLvTAE\n",
            "  ‚è≥ Retrying in 3 seconds...\n",
            "  ‚è≥ Rate limiting: waiting 2.8 seconds...\n",
            "  ‚ùå [Gemini] Attempt 2 failed for 'Doberman Pinscher': 503 Server Error: Service Unavailable for url: https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyADI--hSVZgSqFBKFH4Mbc-i-bCglLvTAE\n",
            "  ‚è≥ Retrying in 6 seconds...\n",
            "  ‚è≥ Rate limiting: waiting 5.9 seconds...\n",
            "  ‚ùå [Gemini] Attempt 3 failed for 'Doberman Pinscher': 503 Server Error: Service Unavailable for url: https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyADI--hSVZgSqFBKFH4Mbc-i-bCglLvTAE\n",
            "  ‚è≥ Retrying in 12 seconds...\n",
            "  ‚è≥ Rate limiting: waiting 11.9 seconds...\n",
            "  ‚ùå [Gemini] Attempt 4 failed for 'Doberman Pinscher': 503 Server Error: Service Unavailable for url: https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyADI--hSVZgSqFBKFH4Mbc-i-bCglLvTAE\n",
            "  ‚è≥ Retrying in 24 seconds...\n",
            "  ‚è≥ Rate limiting: waiting 23.9 seconds...\n",
            "  ‚ùå [Gemini] Attempt 5 failed for 'Doberman Pinscher': 503 Server Error: Service Unavailable for url: https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyADI--hSVZgSqFBKFH4Mbc-i-bCglLvTAE\n",
            "  ‚ùå [Gemini] All attempts failed for 'Doberman Pinscher', using fallback data\n",
            "‚úÖ Successfully processed 'Doberman Pinscher'\n",
            "\n",
            "üìä Progress: 32/45 (Total: 32/45)\n",
            "\n",
            "üîÑ Processing: Brazilian Mastiff\n",
            "==================================================\n",
            "üì° LEVEL 1: Web Scraping\n",
            "  üîç [AKC Scraper] Searching for 'Brazilian Mastiff'...\n",
            "  ‚ö†Ô∏è [AKC] Network error for 'Brazilian Mastiff': 404 Client Error: Not Found for url: https://www.akc.org/dog-breeds/brazilian-mastiff/\n",
            "  üîç [Wikipedia] Searching for 'Brazilian Mastiff'...\n",
            "  ‚úÖ [Wikipedia] Found general info for 'Brazilian Mastiff'\n",
            "ü§ñ LEVEL 2: LLM Enrichment\n",
            "  ü§ñ [Gemini] Enriching data for 'Brazilian Mastiff'...\n",
            "  ‚è≥ Rate limiting: waiting 50.0 seconds...\n",
            "  ‚úÖ [Gemini] Successfully enriched 'Brazilian Mastiff' with 22 features\n",
            "‚úÖ Successfully processed 'Brazilian Mastiff'\n",
            "\n",
            "üìä Progress: 33/45 (Total: 33/45)\n",
            "\n",
            "üîÑ Processing: Boxer\n",
            "==================================================\n",
            "üì° LEVEL 1: Web Scraping\n",
            "  üîç [AKC Scraper] Searching for 'Boxer'...\n",
            "  ‚úÖ [AKC] Found 0 sections for 'Boxer'\n",
            "  üîç [Wikipedia] Searching for 'Boxer'...\n",
            "  ‚è≥ Rate limiting: waiting 0.9 seconds...\n",
            "  ‚úÖ [Wikipedia] Found general info for 'Boxer'\n",
            "ü§ñ LEVEL 2: LLM Enrichment\n",
            "  ü§ñ [Gemini] Enriching data for 'Boxer'...\n",
            "  ‚úÖ [Gemini] Successfully enriched 'Boxer' with 22 features\n",
            "‚úÖ Successfully processed 'Boxer'\n",
            "\n",
            "üìä Progress: 34/45 (Total: 34/45)\n",
            "\n",
            "üîÑ Processing: Dachshund\n",
            "==================================================\n",
            "üì° LEVEL 1: Web Scraping\n",
            "  üîç [AKC Scraper] Searching for 'Dachshund'...\n",
            "  ‚úÖ [AKC] Found 0 sections for 'Dachshund'\n",
            "  üîç [Wikipedia] Searching for 'Dachshund'...\n",
            "  ‚è≥ Rate limiting: waiting 0.9 seconds...\n",
            "  ‚úÖ [Wikipedia] Found general info for 'Dachshund'\n",
            "ü§ñ LEVEL 2: LLM Enrichment\n",
            "  ü§ñ [Gemini] Enriching data for 'Dachshund'...\n",
            "  ‚úÖ [Gemini] Successfully enriched 'Dachshund' with 22 features\n",
            "‚úÖ Successfully processed 'Dachshund'\n",
            "\n",
            "üìä Progress: 35/45 (Total: 35/45)\n",
            "\n",
            "üîÑ Processing: Exotic Shorthair\n",
            "==================================================\n",
            "üì° LEVEL 1: Web Scraping\n",
            "  üîç [AKC Scraper] Searching for 'Exotic Shorthair'...\n",
            "  ‚ö†Ô∏è [AKC] Network error for 'Exotic Shorthair': 404 Client Error: Not Found for url: https://www.akc.org/dog-breeds/exotic-shorthair/\n",
            "  üîç [Wikipedia] Searching for 'Exotic Shorthair'...\n",
            "  ‚è≥ Rate limiting: waiting 0.7 seconds...\n",
            "  ‚úÖ [Wikipedia] Found general info for 'Exotic Shorthair'\n",
            "ü§ñ LEVEL 2: LLM Enrichment\n",
            "  ü§ñ [Gemini] Enriching data for 'Exotic Shorthair'...\n",
            "  ‚ùå [Gemini] Attempt 1 failed for 'Exotic Shorthair': 503 Server Error: Service Unavailable for url: https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyADI--hSVZgSqFBKFH4Mbc-i-bCglLvTAE\n",
            "  ‚è≥ Retrying in 3 seconds...\n",
            "  ‚è≥ Rate limiting: waiting 0.5 seconds...\n",
            "  ‚úÖ [Gemini] Successfully enriched 'Exotic Shorthair' with 22 features\n",
            "‚úÖ Successfully processed 'Exotic Shorthair'\n",
            "üíæ Intermediate save completed (35 breeds processed)\n",
            "\n",
            "üìä Progress: 36/45 (Total: 36/45)\n",
            "\n",
            "üîÑ Processing: Bengal\n",
            "==================================================\n",
            "üì° LEVEL 1: Web Scraping\n",
            "  üîç [AKC Scraper] Searching for 'Bengal'...\n",
            "  ‚ö†Ô∏è [AKC] Network error for 'Bengal': 404 Client Error: Not Found for url: https://www.akc.org/dog-breeds/bengal/\n",
            "  üîç [Wikipedia] Searching for 'Bengal'...\n",
            "  ‚è≥ Rate limiting: waiting 0.4 seconds...\n",
            "  ‚úÖ [Wikipedia] Found general info for 'Bengal'\n",
            "ü§ñ LEVEL 2: LLM Enrichment\n",
            "  ü§ñ [Gemini] Enriching data for 'Bengal'...\n",
            "  ‚úÖ [Gemini] Successfully enriched 'Bengal' with 22 features\n",
            "‚úÖ Successfully processed 'Bengal'\n",
            "\n",
            "üìä Progress: 37/45 (Total: 37/45)\n",
            "\n",
            "üîÑ Processing: Ragdoll\n",
            "==================================================\n",
            "üì° LEVEL 1: Web Scraping\n",
            "  üîç [AKC Scraper] Searching for 'Ragdoll'...\n",
            "  ‚ö†Ô∏è [AKC] Network error for 'Ragdoll': 404 Client Error: Not Found for url: https://www.akc.org/dog-breeds/ragdoll/\n",
            "  üîç [Wikipedia] Searching for 'Ragdoll'...\n",
            "  ‚è≥ Rate limiting: waiting 0.5 seconds...\n",
            "  ‚úÖ [Wikipedia] Found general info for 'Ragdoll'\n",
            "ü§ñ LEVEL 2: LLM Enrichment\n",
            "  ü§ñ [Gemini] Enriching data for 'Ragdoll'...\n",
            "  ‚úÖ [Gemini] Successfully enriched 'Ragdoll' with 22 features\n",
            "‚úÖ Successfully processed 'Ragdoll'\n",
            "\n",
            "üìä Progress: 38/45 (Total: 38/45)\n",
            "\n",
            "üîÑ Processing: Siberian Cat\n",
            "==================================================\n",
            "üì° LEVEL 1: Web Scraping\n",
            "  üîç [AKC Scraper] Searching for 'Siberian Cat'...\n",
            "  ‚ö†Ô∏è [AKC] Network error for 'Siberian Cat': 404 Client Error: Not Found for url: https://www.akc.org/dog-breeds/siberian-cat/\n",
            "  üîç [Wikipedia] Searching for 'Siberian Cat'...\n",
            "  ‚è≥ Rate limiting: waiting 0.6 seconds...\n",
            "  ‚ö†Ô∏è [Wikipedia] Error for 'Siberian Cat': 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Siberian_Cat\n",
            "ü§ñ LEVEL 2: LLM Enrichment\n",
            "  ü§ñ [Gemini] Enriching data for 'Siberian Cat'...\n",
            "  ‚úÖ [Gemini] Successfully enriched 'Siberian Cat' with 22 features\n",
            "‚úÖ Successfully processed 'Siberian Cat'\n",
            "\n",
            "üìä Progress: 39/45 (Total: 39/45)\n",
            "\n",
            "üîÑ Processing: Russian Blue\n",
            "==================================================\n",
            "üì° LEVEL 1: Web Scraping\n",
            "  üîç [AKC Scraper] Searching for 'Russian Blue'...\n",
            "  ‚ö†Ô∏è [AKC] Network error for 'Russian Blue': 404 Client Error: Not Found for url: https://www.akc.org/dog-breeds/russian-blue/\n",
            "  üîç [Wikipedia] Searching for 'Russian Blue'...\n",
            "  ‚è≥ Rate limiting: waiting 2.4 seconds...\n",
            "  ‚úÖ [Wikipedia] Found general info for 'Russian Blue'\n",
            "ü§ñ LEVEL 2: LLM Enrichment\n",
            "  ü§ñ [Gemini] Enriching data for 'Russian Blue'...\n",
            "  ‚ö†Ô∏è [Gemini] Rate limit hit (attempt 1/5)\n",
            "  ‚è≥ Waiting 3 seconds before retry...\n",
            "  ‚è≥ Rate limiting: waiting 2.9 seconds...\n",
            "  ‚ö†Ô∏è [Gemini] Rate limit hit (attempt 2/5)\n",
            "  ‚è≥ Waiting 6 seconds before retry...\n",
            "  ‚è≥ Rate limiting: waiting 5.9 seconds...\n",
            "  ‚ö†Ô∏è [Gemini] Rate limit hit (attempt 3/5)\n",
            "  ‚è≥ Waiting 12 seconds before retry...\n",
            "  ‚è≥ Rate limiting: waiting 11.9 seconds...\n",
            "  ‚ö†Ô∏è [Gemini] Rate limit hit (attempt 4/5)\n",
            "  ‚è≥ Waiting 24 seconds before retry...\n",
            "  ‚è≥ Rate limiting: waiting 23.9 seconds...\n",
            "  ‚ùå [Gemini] Attempt 5 failed for 'Russian Blue': 503 Server Error: Service Unavailable for url: https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyADI--hSVZgSqFBKFH4Mbc-i-bCglLvTAE\n",
            "  ‚ùå [Gemini] All attempts failed for 'Russian Blue', using fallback data\n",
            "‚úÖ Successfully processed 'Russian Blue'\n",
            "\n",
            "üìä Progress: 40/45 (Total: 40/45)\n",
            "\n",
            "üîÑ Processing: Scottish Fold\n",
            "==================================================\n",
            "üì° LEVEL 1: Web Scraping\n",
            "  üîç [AKC Scraper] Searching for 'Scottish Fold'...\n",
            "  ‚ö†Ô∏è [AKC] Network error for 'Scottish Fold': 404 Client Error: Not Found for url: https://www.akc.org/dog-breeds/scottish-fold/\n",
            "  üîç [Wikipedia] Searching for 'Scottish Fold'...\n",
            "  ‚è≥ Rate limiting: waiting 0.4 seconds...\n",
            "  ‚úÖ [Wikipedia] Found general info for 'Scottish Fold'\n",
            "ü§ñ LEVEL 2: LLM Enrichment\n",
            "  ü§ñ [Gemini] Enriching data for 'Scottish Fold'...\n",
            "  ‚è≥ Rate limiting: waiting 44.2 seconds...\n",
            "  ‚ö†Ô∏è [Gemini] Rate limit hit (attempt 1/5)\n",
            "  ‚è≥ Waiting 3 seconds before retry...\n",
            "  ‚è≥ Rate limiting: waiting 56.9 seconds...\n",
            "  ‚ö†Ô∏è [Gemini] Rate limit hit (attempt 2/5)\n",
            "  ‚è≥ Waiting 6 seconds before retry...\n",
            "  ‚è≥ Rate limiting: waiting 53.9 seconds...\n",
            "  ‚ö†Ô∏è [Gemini] Rate limit hit (attempt 3/5)\n",
            "  ‚è≥ Waiting 12 seconds before retry...\n",
            "  ‚è≥ Rate limiting: waiting 47.9 seconds...\n",
            "  ‚ö†Ô∏è [Gemini] Rate limit hit (attempt 4/5)\n",
            "  ‚è≥ Waiting 24 seconds before retry...\n",
            "  ‚è≥ Rate limiting: waiting 35.9 seconds...\n",
            "  ‚ö†Ô∏è [Gemini] Rate limit hit (attempt 5/5)\n",
            "  ‚ùå [Gemini] Attempt 5 failed for 'Scottish Fold': Max retries exceeded for rate limiting\n",
            "  ‚ùå [Gemini] All attempts failed for 'Scottish Fold', using fallback data\n",
            "‚úÖ Successfully processed 'Scottish Fold'\n",
            "üíæ Intermediate save completed (40 breeds processed)\n",
            "\n",
            "üìä Progress: 41/45 (Total: 41/45)\n",
            "\n",
            "üîÑ Processing: Siamese\n",
            "==================================================\n",
            "üì° LEVEL 1: Web Scraping\n",
            "  üîç [AKC Scraper] Searching for 'Siamese'...\n",
            "  ‚ö†Ô∏è [AKC] Network error for 'Siamese': 404 Client Error: Not Found for url: https://www.akc.org/dog-breeds/siamese/\n",
            "  üîç [Wikipedia] Searching for 'Siamese'...\n",
            "  ‚è≥ Rate limiting: waiting 0.7 seconds...\n",
            "  ‚úÖ [Wikipedia] Found general info for 'Siamese'\n",
            "ü§ñ LEVEL 2: LLM Enrichment\n",
            "  ü§ñ [Gemini] Enriching data for 'Siamese'...\n",
            "  ‚è≥ Rate limiting: waiting 58.2 seconds...\n",
            "  ‚úÖ [Gemini] Successfully enriched 'Siamese' with 22 features\n",
            "‚úÖ Successfully processed 'Siamese'\n",
            "\n",
            "üìä Progress: 42/45 (Total: 42/45)\n",
            "\n",
            "üîÑ Processing: American Shorthair\n",
            "==================================================\n",
            "üì° LEVEL 1: Web Scraping\n",
            "  üîç [AKC Scraper] Searching for 'American Shorthair'...\n",
            "  ‚ö†Ô∏è [AKC] Network error for 'American Shorthair': 404 Client Error: Not Found for url: https://www.akc.org/dog-breeds/american-shorthair/\n",
            "  üîç [Wikipedia] Searching for 'American Shorthair'...\n",
            "  ‚è≥ Rate limiting: waiting 0.6 seconds...\n",
            "  ‚úÖ [Wikipedia] Found general info for 'American Shorthair'\n",
            "ü§ñ LEVEL 2: LLM Enrichment\n",
            "  ü§ñ [Gemini] Enriching data for 'American Shorthair'...\n",
            "  ‚ö†Ô∏è [Gemini] Rate limit hit (attempt 1/5)\n",
            "  ‚è≥ Waiting 3 seconds before retry...\n",
            "  ‚è≥ Rate limiting: waiting 2.9 seconds...\n",
            "  ‚ö†Ô∏è [Gemini] Rate limit hit (attempt 2/5)\n",
            "  ‚è≥ Waiting 6 seconds before retry...\n",
            "  ‚è≥ Rate limiting: waiting 5.9 seconds...\n",
            "  ‚ö†Ô∏è [Gemini] Rate limit hit (attempt 3/5)\n",
            "  ‚è≥ Waiting 12 seconds before retry...\n",
            "  ‚è≥ Rate limiting: waiting 11.9 seconds...\n",
            "  ‚ö†Ô∏è [Gemini] Rate limit hit (attempt 4/5)\n",
            "  ‚è≥ Waiting 24 seconds before retry...\n",
            "  ‚è≥ Rate limiting: waiting 23.9 seconds...\n",
            "  ‚ö†Ô∏è [Gemini] Rate limit hit (attempt 5/5)\n",
            "  ‚ùå [Gemini] Attempt 5 failed for 'American Shorthair': Max retries exceeded for rate limiting\n",
            "  ‚ùå [Gemini] All attempts failed for 'American Shorthair', using fallback data\n",
            "‚úÖ Successfully processed 'American Shorthair'\n",
            "\n",
            "üìä Progress: 43/45 (Total: 43/45)\n",
            "\n",
            "üîÑ Processing: Maine Coon\n",
            "==================================================\n",
            "üì° LEVEL 1: Web Scraping\n",
            "  üîç [AKC Scraper] Searching for 'Maine Coon'...\n",
            "  ‚ö†Ô∏è [AKC] Network error for 'Maine Coon': 404 Client Error: Not Found for url: https://www.akc.org/dog-breeds/maine-coon/\n",
            "  üîç [Wikipedia] Searching for 'Maine Coon'...\n",
            "  ‚è≥ Rate limiting: waiting 0.4 seconds...\n",
            "  ‚úÖ [Wikipedia] Found general info for 'Maine Coon'\n",
            "ü§ñ LEVEL 2: LLM Enrichment\n",
            "  ü§ñ [Gemini] Enriching data for 'Maine Coon'...\n",
            "  ‚è≥ Rate limiting: waiting 58.1 seconds...\n",
            "  ‚ö†Ô∏è [Gemini] Rate limit hit (attempt 1/5)\n",
            "  ‚è≥ Waiting 3 seconds before retry...\n",
            "  ‚è≥ Rate limiting: waiting 56.9 seconds...\n",
            "  ‚ö†Ô∏è [Gemini] Rate limit hit (attempt 2/5)\n",
            "  ‚è≥ Waiting 6 seconds before retry...\n",
            "  ‚è≥ Rate limiting: waiting 53.9 seconds...\n",
            "  ‚ö†Ô∏è [Gemini] Rate limit hit (attempt 3/5)\n",
            "  ‚è≥ Waiting 12 seconds before retry...\n",
            "  ‚è≥ Rate limiting: waiting 47.9 seconds...\n",
            "  ‚ö†Ô∏è [Gemini] Rate limit hit (attempt 4/5)\n",
            "  ‚è≥ Waiting 24 seconds before retry...\n",
            "  ‚è≥ Rate limiting: waiting 35.9 seconds...\n",
            "  ‚ö†Ô∏è [Gemini] Rate limit hit (attempt 5/5)\n",
            "  ‚ùå [Gemini] Attempt 5 failed for 'Maine Coon': Max retries exceeded for rate limiting\n",
            "  ‚ùå [Gemini] All attempts failed for 'Maine Coon', using fallback data\n",
            "‚úÖ Successfully processed 'Maine Coon'\n",
            "\n",
            "üìä Progress: 44/45 (Total: 44/45)\n",
            "\n",
            "üîÑ Processing: British Shorthair\n",
            "==================================================\n",
            "üì° LEVEL 1: Web Scraping\n",
            "  üîç [AKC Scraper] Searching for 'British Shorthair'...\n",
            "  ‚ö†Ô∏è [AKC] Network error for 'British Shorthair': 404 Client Error: Not Found for url: https://www.akc.org/dog-breeds/british-shorthair/\n",
            "  üîç [Wikipedia] Searching for 'British Shorthair'...\n",
            "  ‚úÖ [Wikipedia] Found general info for 'British Shorthair'\n",
            "ü§ñ LEVEL 2: LLM Enrichment\n",
            "  ü§ñ [Gemini] Enriching data for 'British Shorthair'...\n",
            "  ‚è≥ Rate limiting: waiting 49.9 seconds...\n",
            "  ‚ö†Ô∏è [Gemini] Rate limit hit (attempt 1/5)\n",
            "  ‚è≥ Waiting 3 seconds before retry...\n",
            "  ‚è≥ Rate limiting: waiting 56.9 seconds...\n",
            "  ‚ö†Ô∏è [Gemini] Rate limit hit (attempt 2/5)\n",
            "  ‚è≥ Waiting 6 seconds before retry...\n",
            "  ‚è≥ Rate limiting: waiting 53.9 seconds...\n",
            "  ‚ö†Ô∏è [Gemini] Rate limit hit (attempt 3/5)\n",
            "  ‚è≥ Waiting 12 seconds before retry...\n",
            "  ‚è≥ Rate limiting: waiting 47.9 seconds...\n",
            "  ‚ö†Ô∏è [Gemini] Rate limit hit (attempt 4/5)\n",
            "  ‚è≥ Waiting 24 seconds before retry...\n",
            "  ‚è≥ Rate limiting: waiting 35.9 seconds...\n",
            "  ‚ö†Ô∏è [Gemini] Rate limit hit (attempt 5/5)\n",
            "  ‚ùå [Gemini] Attempt 5 failed for 'British Shorthair': Max retries exceeded for rate limiting\n",
            "  ‚ùå [Gemini] All attempts failed for 'British Shorthair', using fallback data\n",
            "‚úÖ Successfully processed 'British Shorthair'\n",
            "\n",
            "üìä Progress: 45/45 (Total: 45/45)\n",
            "\n",
            "üîÑ Processing: Persian\n",
            "==================================================\n",
            "üì° LEVEL 1: Web Scraping\n",
            "  üîç [AKC Scraper] Searching for 'Persian'...\n",
            "  ‚ö†Ô∏è [AKC] Network error for 'Persian': 404 Client Error: Not Found for url: https://www.akc.org/dog-breeds/persian/\n",
            "  üîç [Wikipedia] Searching for 'Persian'...\n",
            "  ‚è≥ Rate limiting: waiting 0.4 seconds...\n",
            "  ‚úÖ [Wikipedia] Found general info for 'Persian'\n",
            "ü§ñ LEVEL 2: LLM Enrichment\n",
            "  ü§ñ [Gemini] Enriching data for 'Persian'...\n",
            "  ‚è≥ Rate limiting: waiting 58.3 seconds...\n",
            "  ‚ö†Ô∏è [Gemini] Rate limit hit (attempt 1/5)\n",
            "  ‚è≥ Waiting 3 seconds before retry...\n",
            "  ‚è≥ Rate limiting: waiting 56.9 seconds...\n",
            "  ‚ö†Ô∏è [Gemini] Rate limit hit (attempt 2/5)\n",
            "  ‚è≥ Waiting 6 seconds before retry...\n",
            "  ‚è≥ Rate limiting: waiting 53.9 seconds...\n",
            "  ‚ö†Ô∏è [Gemini] Rate limit hit (attempt 3/5)\n",
            "  ‚è≥ Waiting 12 seconds before retry...\n",
            "  ‚è≥ Rate limiting: waiting 47.9 seconds...\n",
            "  ‚ö†Ô∏è [Gemini] Rate limit hit (attempt 4/5)\n",
            "  ‚è≥ Waiting 24 seconds before retry...\n",
            "  ‚è≥ Rate limiting: waiting 35.9 seconds...\n",
            "  ‚ö†Ô∏è [Gemini] Rate limit hit (attempt 5/5)\n",
            "  ‚ùå [Gemini] Attempt 5 failed for 'Persian': Max retries exceeded for rate limiting\n",
            "  ‚ùå [Gemini] All attempts failed for 'Persian', using fallback data\n",
            "‚úÖ Successfully processed 'Persian'\n",
            "üíæ Intermediate save completed (45 breeds processed)\n",
            "\n",
            "üéâ PIPELINE COMPLETED!\n",
            "============================================================\n",
            "‚è±Ô∏è  Total time: 2080.6 seconds\n",
            "‚úÖ Successfully processed: 45 breeds\n",
            "‚ùå Failed: 0 breeds\n",
            "üíæ Final dataset saved to: 'final_enriched_training_data.json'\n",
            "\n",
            "üéâ Script completed! Check the output files for results.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Pet Dataset Finalization - Missing Value Imputation using Grok LLM\n",
        "====================================================================\n",
        "This script scans the final enriched dataset for any missing values (empty strings or lists)\n",
        "and uses the Grok LLM API (via an OpenAI-compatible endpoint) to intelligently fill them\n",
        "based on the existing context for each breed.\n",
        "\n",
        "This is the final step to create a production-ready dataset.\n",
        "\"\"\"\n",
        "\n",
        "# =============================================================================\n",
        "# SETUP AND INSTALLATION\n",
        "# =============================================================================\n",
        "\n",
        "# Grok's API is OpenAI-compatible, so we use the openai library\n",
        "!pip install -q openai\n",
        "\n",
        "import json\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "from typing import Dict, List, Any\n",
        "from openai import OpenAI\n",
        "\n",
        "# =============================================================================\n",
        "# CONFIGURATION AND CONSTANTS\n",
        "# =============================================================================\n",
        "\n",
        "# File paths\n",
        "INPUT_FILE = 'final_enriched_training_data.json'\n",
        "OUTPUT_FILE = 'Fluffyn.json'\n",
        "\n",
        "# Grok API configuration\n",
        "GROQ_API_BASE_URL = \"https://api.groq.com/openai/v1\"\n",
        "GROQ_MODEL_NAME = \"llama3-8b-8192\" # Using the fast 8b model for this task\n",
        "\n",
        "# =============================================================================\n",
        "# GROK API SETUP\n",
        "# =============================================================================\n",
        "\n",
        "def get_grok_api_key():\n",
        "    \"\"\"Gets the Grok API key from environment variables or user input.\"\"\"\n",
        "    print(\"üîß Getting Groq API key...\")\n",
        "    api_key = os.environ.get(\"GROQ_API_KEY\")\n",
        "    if not api_key:\n",
        "        print(\"‚ö†Ô∏è GROQ_API_KEY not found in environment variables.\")\n",
        "        print(\"Please get your free API key from: https://console.groq.com/keys\")\n",
        "        api_key = input(\"Enter your Groq API key: \").strip()\n",
        "        if not api_key:\n",
        "            raise ValueError(\"Groq API key is required to proceed\")\n",
        "    print(\"‚úÖ Groq API key loaded.\")\n",
        "    return api_key\n",
        "\n",
        "# =============================================================================\n",
        "# CORE LOGIC: Grok-Powered Data Filler\n",
        "# =============================================================================\n",
        "\n",
        "class GrokDataFiller:\n",
        "    \"\"\"\n",
        "    Uses Grok's LLM to intelligently fill missing values in a dataset.\n",
        "    \"\"\"\n",
        "    def __init__(self, api_key: str):\n",
        "        # Initialize the OpenAI client to point to Grok's endpoint\n",
        "        self.client = OpenAI(\n",
        "            api_key=api_key,\n",
        "            base_url=GROQ_API_BASE_URL,\n",
        "        )\n",
        "        print(f\"üß† Grok client initialized with model: {GROQ_MODEL_NAME}\")\n",
        "\n",
        "    def _create_fill_prompt(self, breed_name: str, field_to_fill: str, existing_data: Dict) -> str:\n",
        "        \"\"\"Creates a highly specific prompt to fill one missing field.\"\"\"\n",
        "\n",
        "        # Remove empty fields from existing data to provide clean context\n",
        "        clean_context = {k: v for k, v in existing_data.items() if v}\n",
        "\n",
        "        return f\"\"\"\n",
        "        You are a pet data expert. Your task is to fill in a single missing piece of information for a pet breed profile based on the provided context.\n",
        "\n",
        "        BREED: \"{breed_name}\"\n",
        "\n",
        "        EXISTING DATA CONTEXT:\n",
        "        {json.dumps(clean_context, indent=2)}\n",
        "\n",
        "        MISSING FIELD TO FILL: \"{field_to_fill}\"\n",
        "\n",
        "        Based on all the provided context about the \"{breed_name}\", generate a concise and accurate value for the missing field.\n",
        "\n",
        "        IMPORTANT: Output ONLY the value for the field. Do not say \"The value for the field is...\". Just provide the direct answer. If you are generating a list, provide a Python-style list of strings e.g., [\"item1\", \"item2\"].\n",
        "        \"\"\"\n",
        "\n",
        "    def fill_missing_field(self, breed_name: str, field_to_fill: str, existing_data: Dict) -> Any:\n",
        "        \"\"\"Calls the Grok API to fill a single field, with exponential backoff.\"\"\"\n",
        "        prompt = self._create_fill_prompt(breed_name, field_to_fill, existing_data)\n",
        "\n",
        "        max_retries = 3\n",
        "        base_delay = 5  # seconds\n",
        "\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                print(f\"    ü§ñ Calling Groq for field: '{field_to_fill}'...\")\n",
        "                chat_completion = self.client.chat.completions.create(\n",
        "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                    model=GROQ_MODEL_NAME,\n",
        "                    temperature=0.5, # Be more factual\n",
        "                    max_tokens=256,\n",
        "                )\n",
        "\n",
        "                result = chat_completion.choices[0].message.content.strip()\n",
        "\n",
        "                # Try to parse if it's a list, otherwise return as string\n",
        "                if result.startswith('[') and result.endswith(']'):\n",
        "                    try:\n",
        "                        return json.loads(result)\n",
        "                    except json.JSONDecodeError:\n",
        "                        return result # Return as string if parsing fails\n",
        "\n",
        "                return result\n",
        "\n",
        "            except Exception as e:\n",
        "                # Check for rate limit error (usually a 429 status code in the error message)\n",
        "                if '429' in str(e):\n",
        "                    wait_time = base_delay * (2 ** attempt) + random.uniform(0, 1)\n",
        "                    print(f\"    ‚ö†Ô∏è Rate limit hit. Retrying in {wait_time:.1f} seconds...\")\n",
        "                    time.sleep(wait_time)\n",
        "                else:\n",
        "                    print(f\"    ‚ùå An unexpected error occurred: {e}\")\n",
        "                    return None # Fail on other errors\n",
        "\n",
        "        print(f\"    ‚ùå Failed to get data for '{field_to_fill}' after {max_retries} retries.\")\n",
        "        return None\n",
        "\n",
        "    def process_dataset(self, data: Dict) -> Dict:\n",
        "        \"\"\"Iterates through the dataset and fills all missing values.\"\"\"\n",
        "        final_data = data.copy()\n",
        "        total_breeds = len(final_data)\n",
        "\n",
        "        for i, (breed_name, breed_data) in enumerate(final_data.items(), 1):\n",
        "            print(f\"\\nüîÑ Processing breed {i}/{total_breeds}: {breed_name}\")\n",
        "\n",
        "            # Find fields with missing values (empty string or empty list)\n",
        "            missing_fields = [k for k, v in breed_data.items() if v == \"\" or v == []]\n",
        "\n",
        "            if not missing_fields:\n",
        "                print(\"  ‚úÖ No missing values found. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            print(f\"  üîç Found {len(missing_fields)} missing fields: {missing_fields}\")\n",
        "\n",
        "            for field in missing_fields:\n",
        "                # Pass the current state of breed_data as context\n",
        "                filled_value = self.fill_missing_field(breed_name, field, final_data[breed_name])\n",
        "\n",
        "                if filled_value:\n",
        "                    print(f\"    ‚úÖ Filled '{field}' successfully.\")\n",
        "                    final_data[breed_name][field] = filled_value\n",
        "                else:\n",
        "                    print(f\"    ‚ùå Failed to fill '{field}'. Leaving as is.\")\n",
        "\n",
        "                time.sleep(1) # Add a small delay between field requests\n",
        "\n",
        "        return final_data\n",
        "\n",
        "# =============================================================================\n",
        "# MAIN EXECUTION\n",
        "# =============================================================================\n",
        "def run():\n",
        "    \"\"\"Main function to run the entire pipeline.\"\"\"\n",
        "    print(\"üöÄ STARTING FINAL DATASET IMPUTATION PIPELINE\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    try:\n",
        "        api_key = get_grok_api_key()\n",
        "        filler = GrokDataFiller(api_key=api_key)\n",
        "\n",
        "        print(f\"üìÇ Loading data from '{INPUT_FILE}'...\")\n",
        "        with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
        "            input_data = json.load(f)\n",
        "\n",
        "        final_dataset = filler.process_dataset(input_data)\n",
        "\n",
        "        print(\"\\nüéâ PIPELINE COMPLETED!\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        print(f\"üíæ Saving fully enriched dataset to '{OUTPUT_FILE}'...\")\n",
        "        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
        "            json.dump(final_dataset, f, indent=4, ensure_ascii=False)\n",
        "        print(\"‚úÖ Success! Your final dataset is ready.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå A critical error occurred during the pipeline: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUfKM2Dyz1E0",
        "outputId": "b01c70b7-f7ec-4573-be2b-e7adb49ce305"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ STARTING FINAL DATASET IMPUTATION PIPELINE\n",
            "============================================================\n",
            "üîß Getting Groq API key...\n",
            "‚ö†Ô∏è GROQ_API_KEY not found in environment variables.\n",
            "Please get your free API key from: https://console.groq.com/keys\n",
            "Enter your Groq API key: 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\n",
            "‚úÖ Groq API key loaded.\n",
            "üß† Grok client initialized with model: llama3-8b-8192\n",
            "üìÇ Loading data from 'final_enriched_training_data.json'...\n",
            "\n",
            "üîÑ Processing breed 1/45: Rottweiler\n",
            "  üîç Found 1 missing fields: ['celebrity_owners']\n",
            "    ü§ñ Calling Groq for field: 'celebrity_owners'...\n",
            "    ‚úÖ Filled 'celebrity_owners' successfully.\n",
            "\n",
            "üîÑ Processing breed 2/45: Lhasa Apso\n",
            "  üîç Found 1 missing fields: ['celebrity_owners']\n",
            "    ü§ñ Calling Groq for field: 'celebrity_owners'...\n",
            "    ‚úÖ Filled 'celebrity_owners' successfully.\n",
            "\n",
            "üîÑ Processing breed 3/45: Beagle\n",
            "  üîç Found 1 missing fields: ['celebrity_owners']\n",
            "    ü§ñ Calling Groq for field: 'celebrity_owners'...\n",
            "    ‚úÖ Filled 'celebrity_owners' successfully.\n",
            "\n",
            "üîÑ Processing breed 4/45: American Bulldog\n",
            "  üîç Found 1 missing fields: ['celebrity_owners']\n",
            "    ü§ñ Calling Groq for field: 'celebrity_owners'...\n",
            "    ‚úÖ Filled 'celebrity_owners' successfully.\n",
            "\n",
            "üîÑ Processing breed 5/45: Bull Mastiff\n",
            "  üîç Found 1 missing fields: ['celebrity_owners']\n",
            "    ü§ñ Calling Groq for field: 'celebrity_owners'...\n",
            "    ‚úÖ Filled 'celebrity_owners' successfully.\n",
            "\n",
            "üîÑ Processing breed 6/45: Chow Chow\n",
            "  üîç Found 1 missing fields: ['celebrity_owners']\n",
            "    ü§ñ Calling Groq for field: 'celebrity_owners'...\n",
            "    ‚úÖ Filled 'celebrity_owners' successfully.\n",
            "\n",
            "üîÑ Processing breed 7/45: German Shepherd\n",
            "  üîç Found 1 missing fields: ['celebrity_owners']\n",
            "    ü§ñ Calling Groq for field: 'celebrity_owners'...\n",
            "    ‚úÖ Filled 'celebrity_owners' successfully.\n",
            "\n",
            "üîÑ Processing breed 8/45: Alaskan Malamute\n",
            "  üîç Found 1 missing fields: ['celebrity_owners']\n",
            "    ü§ñ Calling Groq for field: 'celebrity_owners'...\n",
            "    ‚úÖ Filled 'celebrity_owners' successfully.\n",
            "\n",
            "üîÑ Processing breed 9/45: Siberian Husky\n",
            "  üîç Found 1 missing fields: ['celebrity_owners']\n",
            "    ü§ñ Calling Groq for field: 'celebrity_owners'...\n",
            "    ‚úÖ Filled 'celebrity_owners' successfully.\n",
            "\n",
            "üîÑ Processing breed 10/45: Corgi\n",
            "  üîç Found 1 missing fields: ['celebrity_owners']\n",
            "    ü§ñ Calling Groq for field: 'celebrity_owners'...\n",
            "    ‚úÖ Filled 'celebrity_owners' successfully.\n",
            "\n",
            "üîÑ Processing breed 11/45: Chihuahua\n",
            "  üîç Found 1 missing fields: ['celebrity_owners']\n",
            "    ü§ñ Calling Groq for field: 'celebrity_owners'...\n",
            "    ‚úÖ Filled 'celebrity_owners' successfully.\n",
            "\n",
            "üîÑ Processing breed 12/45: Tibetan Mastiff\n",
            "  üîç Found 1 missing fields: ['celebrity_owners']\n",
            "    ü§ñ Calling Groq for field: 'celebrity_owners'...\n",
            "    ‚úÖ Filled 'celebrity_owners' successfully.\n",
            "\n",
            "üîÑ Processing breed 13/45: Pomeranian\n",
            "  üîç Found 1 missing fields: ['celebrity_owners']\n",
            "    ü§ñ Calling Groq for field: 'celebrity_owners'...\n",
            "    ‚úÖ Filled 'celebrity_owners' successfully.\n",
            "\n",
            "üîÑ Processing breed 14/45: Golden Retriever\n",
            "  üîç Found 1 missing fields: ['celebrity_owners']\n",
            "    ü§ñ Calling Groq for field: 'celebrity_owners'...\n",
            "    ‚úÖ Filled 'celebrity_owners' successfully.\n",
            "\n",
            "üîÑ Processing breed 15/45: French Mastiff\n",
            "  üîç Found 1 missing fields: ['celebrity_owners']\n",
            "    ü§ñ Calling Groq for field: 'celebrity_owners'...\n",
            "    ‚úÖ Filled 'celebrity_owners' successfully.\n",
            "\n",
            "üîÑ Processing breed 16/45: English Mastiff\n",
            "  üîç Found 1 missing fields: ['celebrity_owners']\n",
            "    ü§ñ Calling Groq for field: 'celebrity_owners'...\n",
            "    ‚úÖ Filled 'celebrity_owners' successfully.\n",
            "\n",
            "üîÑ Processing breed 17/45: Saint Bernard\n",
            "  üîç Found 1 missing fields: ['celebrity_owners']\n",
            "    ü§ñ Calling Groq for field: 'celebrity_owners'...\n",
            "    ‚úÖ Filled 'celebrity_owners' successfully.\n",
            "\n",
            "üîÑ Processing breed 18/45: Cane Corso\n",
            "  üîç Found 1 missing fields: ['celebrity_owners']\n",
            "    ü§ñ Calling Groq for field: 'celebrity_owners'...\n",
            "    ‚úÖ Filled 'celebrity_owners' successfully.\n",
            "\n",
            "üîÑ Processing breed 19/45: Shih Tzu\n",
            "  üîç Found 1 missing fields: ['celebrity_owners']\n",
            "    ü§ñ Calling Groq for field: 'celebrity_owners'...\n",
            "    ‚úÖ Filled 'celebrity_owners' successfully.\n",
            "\n",
            "üîÑ Processing breed 20/45: Border Collie\n",
            "  üîç Found 1 missing fields: ['celebrity_owners']\n",
            "    ü§ñ Calling Groq for field: 'celebrity_owners'...\n",
            "    ‚úÖ Filled 'celebrity_owners' successfully.\n",
            "\n",
            "üîÑ Processing breed 21/45: Great Dane\n",
            "  üîç Found 1 missing fields: ['celebrity_owners']\n",
            "    ü§ñ Calling Groq for field: 'celebrity_owners'...\n",
            "    ‚úÖ Filled 'celebrity_owners' successfully.\n",
            "\n",
            "üîÑ Processing breed 22/45: Cocker Spaniel\n",
            "  üîç Found 1 missing fields: ['celebrity_owners']\n",
            "    ü§ñ Calling Groq for field: 'celebrity_owners'...\n",
            "    ‚úÖ Filled 'celebrity_owners' successfully.\n",
            "\n",
            "üîÑ Processing breed 23/45: French Bulldog\n",
            "  üîç Found 1 missing fields: ['celebrity_owners']\n",
            "    ü§ñ Calling Groq for field: 'celebrity_owners'...\n",
            "    ‚úÖ Filled 'celebrity_owners' successfully.\n",
            "\n",
            "üîÑ Processing breed 24/45: Labrador Retriever\n",
            "  üîç Found 1 missing fields: ['celebrity_owners']\n",
            "    ü§ñ Calling Groq for field: 'celebrity_owners'...\n",
            "    ‚úÖ Filled 'celebrity_owners' successfully.\n",
            "\n",
            "üîÑ Processing breed 25/45: English Bulldog\n",
            "  üîç Found 1 missing fields: ['celebrity_owners']\n",
            "    ü§ñ Calling Groq for field: 'celebrity_owners'...\n",
            "    ‚úÖ Filled 'celebrity_owners' successfully.\n",
            "\n",
            "üîÑ Processing breed 26/45: Shiba Inu\n",
            "  üîç Found 1 missing fields: ['celebrity_owners']\n",
            "    ü§ñ Calling Groq for field: 'celebrity_owners'...\n",
            "    ‚úÖ Filled 'celebrity_owners' successfully.\n",
            "\n",
            "üîÑ Processing breed 27/45: Pug\n",
            "  üîç Found 1 missing fields: ['celebrity_owners']\n",
            "    ü§ñ Calling Groq for field: 'celebrity_owners'...\n",
            "    ‚úÖ Filled 'celebrity_owners' successfully.\n",
            "\n",
            "üîÑ Processing breed 28/45: Neapolitan Mastiff\n",
            "  üîç Found 1 missing fields: ['celebrity_owners']\n",
            "    ü§ñ Calling Groq for field: 'celebrity_owners'...\n",
            "    ‚úÖ Filled 'celebrity_owners' successfully.\n",
            "\n",
            "üîÑ Processing breed 29/45: Maltese\n",
            "  üîç Found 1 missing fields: ['celebrity_owners']\n",
            "    ü§ñ Calling Groq for field: 'celebrity_owners'...\n",
            "    ‚úÖ Filled 'celebrity_owners' successfully.\n",
            "\n",
            "üîÑ Processing breed 30/45: Poodle\n",
            "  üîç Found 1 missing fields: ['celebrity_owners']\n",
            "    ü§ñ Calling Groq for field: 'celebrity_owners'...\n",
            "    ‚úÖ Filled 'celebrity_owners' successfully.\n",
            "\n",
            "üîÑ Processing breed 31/45: Doberman Pinscher\n",
            "  üîç Found 2 missing fields: ['common_health_concerns', 'recommended_health_tests']\n",
            "    ü§ñ Calling Groq for field: 'common_health_concerns'...\n",
            "    ‚úÖ Filled 'common_health_concerns' successfully.\n",
            "    ü§ñ Calling Groq for field: 'recommended_health_tests'...\n",
            "    ‚úÖ Filled 'recommended_health_tests' successfully.\n",
            "\n",
            "üîÑ Processing breed 32/45: Brazilian Mastiff\n",
            "  üîç Found 1 missing fields: ['celebrity_owners']\n",
            "    ü§ñ Calling Groq for field: 'celebrity_owners'...\n",
            "    ‚úÖ Filled 'celebrity_owners' successfully.\n",
            "\n",
            "üîÑ Processing breed 33/45: Boxer\n",
            "  üîç Found 1 missing fields: ['celebrity_owners']\n",
            "    ü§ñ Calling Groq for field: 'celebrity_owners'...\n",
            "    ‚úÖ Filled 'celebrity_owners' successfully.\n",
            "\n",
            "üîÑ Processing breed 34/45: Dachshund\n",
            "  üîç Found 1 missing fields: ['celebrity_owners']\n",
            "    ü§ñ Calling Groq for field: 'celebrity_owners'...\n",
            "    ‚úÖ Filled 'celebrity_owners' successfully.\n",
            "\n",
            "üîÑ Processing breed 35/45: Exotic Shorthair\n",
            "  üîç Found 3 missing fields: ['weather_and_climate', 'stinkiness', 'celebrity_owners']\n",
            "    ü§ñ Calling Groq for field: 'weather_and_climate'...\n",
            "    ‚úÖ Filled 'weather_and_climate' successfully.\n",
            "    ü§ñ Calling Groq for field: 'stinkiness'...\n",
            "    ‚úÖ Filled 'stinkiness' successfully.\n",
            "    ü§ñ Calling Groq for field: 'celebrity_owners'...\n",
            "    ‚úÖ Filled 'celebrity_owners' successfully.\n",
            "\n",
            "üîÑ Processing breed 36/45: Bengal\n",
            "  üîç Found 4 missing fields: ['daily_food_consumption', 'weather_and_climate', 'stinkiness', 'celebrity_owners']\n",
            "    ü§ñ Calling Groq for field: 'daily_food_consumption'...\n",
            "    ‚úÖ Filled 'daily_food_consumption' successfully.\n",
            "    ü§ñ Calling Groq for field: 'weather_and_climate'...\n",
            "    ‚úÖ Filled 'weather_and_climate' successfully.\n",
            "    ü§ñ Calling Groq for field: 'stinkiness'...\n",
            "    ‚úÖ Filled 'stinkiness' successfully.\n",
            "    ü§ñ Calling Groq for field: 'celebrity_owners'...\n",
            "    ‚úÖ Filled 'celebrity_owners' successfully.\n",
            "\n",
            "üîÑ Processing breed 37/45: Ragdoll\n",
            "  üîç Found 3 missing fields: ['weather_and_climate', 'stinkiness', 'celebrity_owners']\n",
            "    ü§ñ Calling Groq for field: 'weather_and_climate'...\n",
            "    ‚úÖ Filled 'weather_and_climate' successfully.\n",
            "    ü§ñ Calling Groq for field: 'stinkiness'...\n",
            "    ‚úÖ Filled 'stinkiness' successfully.\n",
            "    ü§ñ Calling Groq for field: 'celebrity_owners'...\n",
            "    ‚úÖ Filled 'celebrity_owners' successfully.\n",
            "\n",
            "üîÑ Processing breed 38/45: Siberian Cat\n",
            "  üîç Found 4 missing fields: ['daily_food_consumption', 'weather_and_climate', 'stinkiness', 'celebrity_owners']\n",
            "    ü§ñ Calling Groq for field: 'daily_food_consumption'...\n",
            "    ‚úÖ Filled 'daily_food_consumption' successfully.\n",
            "    ü§ñ Calling Groq for field: 'weather_and_climate'...\n",
            "    ‚úÖ Filled 'weather_and_climate' successfully.\n",
            "    ü§ñ Calling Groq for field: 'stinkiness'...\n",
            "    ‚úÖ Filled 'stinkiness' successfully.\n",
            "    ü§ñ Calling Groq for field: 'celebrity_owners'...\n",
            "    ‚úÖ Filled 'celebrity_owners' successfully.\n",
            "\n",
            "üîÑ Processing breed 39/45: Russian Blue\n",
            "  üîç Found 5 missing fields: ['daily_food_consumption', 'weather_and_climate', 'stinkiness', 'common_health_concerns', 'recommended_health_tests']\n",
            "    ü§ñ Calling Groq for field: 'daily_food_consumption'...\n",
            "    ‚úÖ Filled 'daily_food_consumption' successfully.\n",
            "    ü§ñ Calling Groq for field: 'weather_and_climate'...\n",
            "    ‚úÖ Filled 'weather_and_climate' successfully.\n",
            "    ü§ñ Calling Groq for field: 'stinkiness'...\n",
            "    ‚úÖ Filled 'stinkiness' successfully.\n",
            "    ü§ñ Calling Groq for field: 'common_health_concerns'...\n",
            "    ‚úÖ Filled 'common_health_concerns' successfully.\n",
            "    ü§ñ Calling Groq for field: 'recommended_health_tests'...\n",
            "    ‚úÖ Filled 'recommended_health_tests' successfully.\n",
            "\n",
            "üîÑ Processing breed 40/45: Scottish Fold\n",
            "  üîç Found 5 missing fields: ['daily_food_consumption', 'weather_and_climate', 'stinkiness', 'common_health_concerns', 'recommended_health_tests']\n",
            "    ü§ñ Calling Groq for field: 'daily_food_consumption'...\n",
            "    ‚úÖ Filled 'daily_food_consumption' successfully.\n",
            "    ü§ñ Calling Groq for field: 'weather_and_climate'...\n",
            "    ‚úÖ Filled 'weather_and_climate' successfully.\n",
            "    ü§ñ Calling Groq for field: 'stinkiness'...\n",
            "    ‚úÖ Filled 'stinkiness' successfully.\n",
            "    ü§ñ Calling Groq for field: 'common_health_concerns'...\n",
            "    ‚úÖ Filled 'common_health_concerns' successfully.\n",
            "    ü§ñ Calling Groq for field: 'recommended_health_tests'...\n",
            "    ‚úÖ Filled 'recommended_health_tests' successfully.\n",
            "\n",
            "üîÑ Processing breed 41/45: Siamese\n",
            "  üîç Found 4 missing fields: ['daily_food_consumption', 'weather_and_climate', 'stinkiness', 'celebrity_owners']\n",
            "    ü§ñ Calling Groq for field: 'daily_food_consumption'...\n",
            "    ‚úÖ Filled 'daily_food_consumption' successfully.\n",
            "    ü§ñ Calling Groq for field: 'weather_and_climate'...\n",
            "    ‚úÖ Filled 'weather_and_climate' successfully.\n",
            "    ü§ñ Calling Groq for field: 'stinkiness'...\n",
            "    ‚úÖ Filled 'stinkiness' successfully.\n",
            "    ü§ñ Calling Groq for field: 'celebrity_owners'...\n",
            "    ‚úÖ Filled 'celebrity_owners' successfully.\n",
            "\n",
            "üîÑ Processing breed 42/45: American Shorthair\n",
            "  üîç Found 4 missing fields: ['weather_and_climate', 'stinkiness', 'common_health_concerns', 'recommended_health_tests']\n",
            "    ü§ñ Calling Groq for field: 'weather_and_climate'...\n",
            "    ‚úÖ Filled 'weather_and_climate' successfully.\n",
            "    ü§ñ Calling Groq for field: 'stinkiness'...\n",
            "    ‚úÖ Filled 'stinkiness' successfully.\n",
            "    ü§ñ Calling Groq for field: 'common_health_concerns'...\n",
            "    ‚úÖ Filled 'common_health_concerns' successfully.\n",
            "    ü§ñ Calling Groq for field: 'recommended_health_tests'...\n",
            "    ‚úÖ Filled 'recommended_health_tests' successfully.\n",
            "\n",
            "üîÑ Processing breed 43/45: Maine Coon\n",
            "  üîç Found 4 missing fields: ['weather_and_climate', 'stinkiness', 'common_health_concerns', 'recommended_health_tests']\n",
            "    ü§ñ Calling Groq for field: 'weather_and_climate'...\n",
            "    ‚úÖ Filled 'weather_and_climate' successfully.\n",
            "    ü§ñ Calling Groq for field: 'stinkiness'...\n",
            "    ‚úÖ Filled 'stinkiness' successfully.\n",
            "    ü§ñ Calling Groq for field: 'common_health_concerns'...\n",
            "    ‚úÖ Filled 'common_health_concerns' successfully.\n",
            "    ü§ñ Calling Groq for field: 'recommended_health_tests'...\n",
            "    ‚úÖ Filled 'recommended_health_tests' successfully.\n",
            "\n",
            "üîÑ Processing breed 44/45: British Shorthair\n",
            "  üîç Found 4 missing fields: ['weather_and_climate', 'stinkiness', 'common_health_concerns', 'recommended_health_tests']\n",
            "    ü§ñ Calling Groq for field: 'weather_and_climate'...\n",
            "    ‚úÖ Filled 'weather_and_climate' successfully.\n",
            "    ü§ñ Calling Groq for field: 'stinkiness'...\n",
            "    ‚úÖ Filled 'stinkiness' successfully.\n",
            "    ü§ñ Calling Groq for field: 'common_health_concerns'...\n",
            "    ‚úÖ Filled 'common_health_concerns' successfully.\n",
            "    ü§ñ Calling Groq for field: 'recommended_health_tests'...\n",
            "    ‚úÖ Filled 'recommended_health_tests' successfully.\n",
            "\n",
            "üîÑ Processing breed 45/45: Persian\n",
            "  üîç Found 4 missing fields: ['weather_and_climate', 'stinkiness', 'common_health_concerns', 'recommended_health_tests']\n",
            "    ü§ñ Calling Groq for field: 'weather_and_climate'...\n",
            "    ‚úÖ Filled 'weather_and_climate' successfully.\n",
            "    ü§ñ Calling Groq for field: 'stinkiness'...\n",
            "    ‚úÖ Filled 'stinkiness' successfully.\n",
            "    ü§ñ Calling Groq for field: 'common_health_concerns'...\n",
            "    ‚úÖ Filled 'common_health_concerns' successfully.\n",
            "    ü§ñ Calling Groq for field: 'recommended_health_tests'...\n",
            "    ‚úÖ Filled 'recommended_health_tests' successfully.\n",
            "\n",
            "üéâ PIPELINE COMPLETED!\n",
            "============================================================\n",
            "üíæ Saving fully enriched dataset to 'final_enrich__finally.json'...\n",
            "‚úÖ Success! Your final dataset is ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5RZwVIuDvXd6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}